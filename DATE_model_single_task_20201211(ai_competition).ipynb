{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 저가신고선별 (DATE model을 single task로 변경)\n",
    "* Single Task (저가신고 적발)용으로 변경, 적발 후 추가세수 예측 부분 삭제 (2020.12.11.)\n",
    "* AI 문제해결과정 참고용\n",
    "\n",
    "\n",
    "# Manual of DATE (Duat-Attentive-Tree-aware-Embedding) model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Updated on: 2020. 6. 3.\n",
    "* Written by Yeon Soo Choi, Technical Officer, Research Unit, WCO\n",
    "* Original research paper\n",
    "    * Title: DATE: Dual Attentive Tree-aware Embedding for Customs Frauds Detection\n",
    "    * Authors\n",
    "        - IBS: Sundong Kim, Karandeep Singh, Meeyoung Cha\n",
    "        - NCKU: Yu-Che Tsai, Cheng-Te Li\n",
    "        - WCO: Yeon Soo Choi\n",
    "        - NCS: Etim Ibok\n",
    "    * Source: https://github.com/Roytsai27/Dual-Attentive-Tree-aware-Embedding\n",
    "        - All the python codes are included in this one notebook.\n",
    "* Data: T import data (xxx at transaction-item level)\n",
    "\n",
    "\n",
    "## Summary (in non-technical terms)\n",
    "\n",
    "**DATE (Dual-Attentive-Tree-aware-Embedding)** is the neural networks model to detect undervalued imports.  \n",
    "\n",
    "Imagine that you **(\"neural networks\")** are the head of a Customs targeting centre composed of **100 risk analysts (\"decision trees\")**. For a given import, you task the analysts with reporting **the probability of undervaluation** and **the estimate of additional revenue from the inspection (\"dual-task\")**.  \n",
    "\n",
    "How would you put 100 different reports together in making your final decision?\n",
    "Simply averaging their predictions may neglect some valuable information hidden in 100 reports. The DATE model help you keep all the information while paying more **ATTENTION** to specific pieces. Firstly, if there are a majority group of reports significantly similar to each other, you may pay more **ATTENTION** to those reports. Secondly, if you have analysts specialized in the specific HS code and importer of the given import, you may pay more **ATTENTION** to their reports. In the end, your final decision reflects the reports attracting your higher attention. \n",
    "\n",
    "WCO press release is available at:  \n",
    "http://www.wcoomd.org/en/media/newsroom/2020/may/wco-bacuda-experts-develop-and-share-a-neural-network-model.aspx\n",
    "\n",
    "## Summary (in technical terms)\n",
    "\n",
    "Now, lets take an overview of the model with some technical terms. For a given import, the DATE model works in the following steps;\n",
    "\n",
    "* **XGBoost**: The model passes the import into a XGBoost model which constructs multiple(eg. 100) decision trees. \n",
    "* **Embedding**: Each tree's decision (decision path, leaf-id) is transformed into a set of numbers to be fed into neural networks. \n",
    "* **Multi-head Self-attention**\n",
    "    - Self-attention: The numeric value (importance, weight) of each leaf-id is adjusted based on its correlation/interaction with other leaf-ids.\n",
    "    - Multi-head: Self-iteration is repeated in multiple times to achieve its robustness.\n",
    "* **Attention**: The numeric values (importance, weight) of each leaf-id is re-adjusted based on its correlation/interaction with the given importer-id and item-id(HScode).\n",
    "\n",
    "## OUTLINE\n",
    "* [Part 1. Preprocess data](#part1)\n",
    "* [Part 2. XGBoost model](#part2)\n",
    "* [Part 3. DATE (XGBoost + Neural Networks + Attention)](#part3)\n",
    "* [Part 4. Evaluation](#part4)\n",
    "* [Part 5. Comparison with XGBoost + Logistic Regression model](#part5)\n",
    "* [Part 6. Practice of functions](#part6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1. Preprocess <a id='part1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Set environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import pickle\n",
    "pd.set_option('display.max_columns',100)\n",
    "from collections import defaultdict\n",
    "from itertools import islice, combinations\n",
    "from datetime import datetime as dt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Load and clean data\n",
    "### 1.2.1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = pd.read_csv(\"가상데이터_훈련_테스트.csv\", encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_raw.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2. Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['year', 'month', 'day', 'OFFICE', 'IMPORTER.TIN', 'TARIFF.CODE',\n",
       "       'DECLARANT.CODE', 'ORIGIN.CODE', 'CIF_USD_EQUIVALENT', 'QUANTITY',\n",
       "       'GROSS.WEIGHT', 'TOTAL.TAXES.USD', 'RAISED_TAX_AMOUNT_USD', 'illicit'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={\"CIF_USD_EQUIVALENT\": \"CIF\", \n",
    "                        \"TOTAL.TAXES.USD\": \"TOTAL.TAXES\",\n",
    "                        \"RAISED_TAX_AMOUNT_USD\":'RAISED_TAX_AMOUNT'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>OFFICE</th>\n",
       "      <th>IMPORTER.TIN</th>\n",
       "      <th>TARIFF.CODE</th>\n",
       "      <th>DECLARANT.CODE</th>\n",
       "      <th>ORIGIN.CODE</th>\n",
       "      <th>CIF</th>\n",
       "      <th>QUANTITY</th>\n",
       "      <th>GROSS.WEIGHT</th>\n",
       "      <th>TOTAL.TAXES</th>\n",
       "      <th>RAISED_TAX_AMOUNT</th>\n",
       "      <th>illicit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>739533</th>\n",
       "      <td>2013</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>OFFICE40</td>\n",
       "      <td>IMP156950</td>\n",
       "      <td>6912001000</td>\n",
       "      <td>DEC2253</td>\n",
       "      <td>CNTRY759</td>\n",
       "      <td>1162.408000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3967.506162</td>\n",
       "      <td>2888.563637</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>739534</th>\n",
       "      <td>2014</td>\n",
       "      <td>11</td>\n",
       "      <td>30</td>\n",
       "      <td>OFFICE59</td>\n",
       "      <td>IMP672344</td>\n",
       "      <td>8452290000</td>\n",
       "      <td>DEC6450</td>\n",
       "      <td>CNTRY759</td>\n",
       "      <td>32942.095158</td>\n",
       "      <td>133.0</td>\n",
       "      <td>50459.331136</td>\n",
       "      <td>87.490876</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>739535</th>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>OFFICE51</td>\n",
       "      <td>IMP496378</td>\n",
       "      <td>8703322900</td>\n",
       "      <td>DEC5159</td>\n",
       "      <td>CNTRY334</td>\n",
       "      <td>1381.442629</td>\n",
       "      <td>88.0</td>\n",
       "      <td>35176.595058</td>\n",
       "      <td>195.792678</td>\n",
       "      <td>0.000407</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>739536</th>\n",
       "      <td>2014</td>\n",
       "      <td>6</td>\n",
       "      <td>28</td>\n",
       "      <td>OFFICE92</td>\n",
       "      <td>IMP633139</td>\n",
       "      <td>8703222000</td>\n",
       "      <td>DEC7849</td>\n",
       "      <td>CNTRY759</td>\n",
       "      <td>7800.389913</td>\n",
       "      <td>1.0</td>\n",
       "      <td>992.750051</td>\n",
       "      <td>1708.778713</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>739537</th>\n",
       "      <td>2013</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>OFFICE59</td>\n",
       "      <td>IMP824798</td>\n",
       "      <td>8703212000</td>\n",
       "      <td>DEC7804</td>\n",
       "      <td>CNTRY615</td>\n",
       "      <td>941.242517</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1414.405081</td>\n",
       "      <td>360.172274</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        year  month  day    OFFICE IMPORTER.TIN  TARIFF.CODE DECLARANT.CODE  \\\n",
       "739533  2013      5   12  OFFICE40    IMP156950   6912001000        DEC2253   \n",
       "739534  2014     11   30  OFFICE59    IMP672344   8452290000        DEC6450   \n",
       "739535  2013      1   27  OFFICE51    IMP496378   8703322900        DEC5159   \n",
       "739536  2014      6   28  OFFICE92    IMP633139   8703222000        DEC7849   \n",
       "739537  2013     12   13  OFFICE59    IMP824798   8703212000        DEC7804   \n",
       "\n",
       "       ORIGIN.CODE           CIF  QUANTITY  GROSS.WEIGHT  TOTAL.TAXES  \\\n",
       "739533    CNTRY759   1162.408000       1.0   3967.506162  2888.563637   \n",
       "739534    CNTRY759  32942.095158     133.0  50459.331136    87.490876   \n",
       "739535    CNTRY334   1381.442629      88.0  35176.595058   195.792678   \n",
       "739536    CNTRY759   7800.389913       1.0    992.750051  1708.778713   \n",
       "739537    CNTRY615    941.242517      18.0   1414.405081   360.172274   \n",
       "\n",
       "        RAISED_TAX_AMOUNT  illicit  \n",
       "739533           0.000000        0  \n",
       "739534           0.000000        0  \n",
       "739535           0.000407        1  \n",
       "739536           0.000000        0  \n",
       "739537           0.000000        0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish loading data...\n"
     ]
    }
   ],
   "source": [
    "print(\"Finish loading data...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Define functions to be used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1. \"merge_attributes\" function [(link to practice)](#practice1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This \"merge_attributes\" fuction is to create a new categorical variable by combining multiple existing categorical variables into one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_attributes(df: pd.DataFrame, *args: str) -> None: \n",
    "    # Note: \"*args\" represents multiple arguments, i.e. multiple variable names could come. \n",
    "    # Note: \"-> None\" represents that this function returns None (i.e. type annotation)\n",
    "    \"\"\"\n",
    "    dtype df: dataframe\n",
    "    dtype *args: strings (attribute names that want to be combined)\n",
    "    \"\"\"\n",
    "    # To set data type of each argument as string\n",
    "    iterables = [df[arg].astype(str) for arg in args] \n",
    "    # To name the newly combined variable/column\n",
    "    columnName = '&'.join([*args]) \n",
    "    # To create a column for the combined variable\n",
    "    fs = [''.join([v for v in var]) for var in zip(*iterables)] # \"*\" represents \"unzip\"\n",
    "    df.loc[:, columnName] = fs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2. \"preprocess\" function [(link to practice)](#practice2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This fuction is to;\n",
    "* generate additional features such as unitprice, weight-unitprice and effective tariff rate;\n",
    "* merge some attributes, using the above \"merge_attributes\" function; and \n",
    "* generate date-related features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Note: \"-> pd.DataFrame\" represents that this function returns a dataframe.\n",
    "    \"\"\"\n",
    "    dtype df: dataframe\n",
    "    rtype df: dataframe\n",
    "    \"\"\"\n",
    "    df = df.dropna(subset=['CIF', 'TOTAL.TAXES'])\n",
    "    df.loc[:, 'Unitprice'] = df['CIF']/df['QUANTITY']\n",
    "    df.loc[:, 'WUnitprice'] = df['CIF']/df['GROSS.WEIGHT']\n",
    "    df.loc[:, 'TaxRatio'] = df['TOTAL.TAXES'] / df['CIF']\n",
    "    df.loc[:, 'TaxUnitquantity'] = df['TOTAL.TAXES'] / df['QUANTITY']\n",
    "    df.loc[:, 'HS6'] = df['TARIFF.CODE'].apply(lambda x: int(x // 10000)) #HS10digit\n",
    "    df.loc[:, 'HS4'] = df['HS6'].apply(lambda x: int(x // 100))\n",
    "    df.loc[:, 'HS2'] = df['HS4'].apply(lambda x: int(x // 100))\n",
    "    \n",
    "    # Made a general function \"merge_attributes\" for supporting any combination    \n",
    "    merge_attributes(df, 'HS6','ORIGIN.CODE')\n",
    "    merge_attributes(df, 'OFFICE','IMPORTER.TIN')\n",
    "    merge_attributes(df, 'OFFICE','HS6')\n",
    "    merge_attributes(df, 'OFFICE','ORIGIN.CODE')\n",
    "    # another way of combining features\n",
    "    #df.loc[:, 'HS6.Origin'] = [str(i)+'&'+j for i, j in zip(df['HS6'], df['ORIGIN'])]\n",
    "    \n",
    "    \n",
    "    '''# Day of Year of SGD.DATE\n",
    "    tmp2 = {}\n",
    "    for date in set(df['SGD.DATE']):\n",
    "        tmp2[date] = dt.strptime(date, '%Y%m%d') # Capital letter Y\n",
    "    tmp_day = {}\n",
    "    tmp_week = {}\n",
    "    tmp_month = {}\n",
    "    yearStart = dt(tmp2[date].date().year, 1, 1)\n",
    "    for item in tmp2:\n",
    "        tmp_day[item] = (tmp2[item] - yearStart).days\n",
    "        tmp_week[item] = int(tmp_day[item] / 7)\n",
    "        tmp_month[item] = int(tmp_day[item] / 30)\n",
    "        \n",
    "    df.loc[:, 'SGD.DayofYear'] = df['SGD.DATE'].apply(lambda x: tmp_day[x])\n",
    "    df.loc[:, 'SGD.WeekofYear'] = df['SGD.DATE'].apply(lambda x: tmp_week[x])\n",
    "    df.loc[:, 'SGD.MonthofYear'] = df['SGD.DATE'].apply(lambda x: tmp_month[x])'''\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.3. \"find_risk_profile\" function [(link to practice)](#practice3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is to identify/calculate risk-profiling indicators of the features;\n",
    "* option 1 (topk): Lists of top n high-risk categories in the features\n",
    "* option 2 (ratio): illicit ratio of categories in the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_risk_profile(df: pd.DataFrame, \n",
    "                      feature: str, \n",
    "                      topk_ratio: float, \n",
    "                      adj: float, \n",
    "                      option: str) -> list or dict:\n",
    "    \"\"\"\n",
    "    dtype feature: str\n",
    "    dtype topk_ratio: float (range: 0-1)\n",
    "    dtype adj: float (to modify the mean)\n",
    "    dtype option: str ('topk', 'ratio')\n",
    "    rtype: list(option='topk') or dict(option='ratio')\n",
    "    \n",
    "    The option topk is usually better than the ratio because of overfitting.\n",
    "    \"\"\"\n",
    "\n",
    "    # Top-k suspicious item flagging\n",
    "    if option == 'topk':\n",
    "        # Group data by a specified feature(vairable)\n",
    "        total_cnt = df.groupby([feature])['illicit']\n",
    "        # Set the number of entities to be included a black list.\n",
    "        nrisky_profile = int(topk_ratio*len(total_cnt))+1\n",
    "        # For each entity, calculate 'total number of frauds' divided by 'total number of imports' \n",
    "        adj_prob_illicit = total_cnt.sum() / (total_cnt.count()+adj)  # Smoothed mean\n",
    "        return list(adj_prob_illicit.sort_values(ascending=False).head(nrisky_profile).index)\n",
    "    \n",
    "    # Illicit-ratio encoding (Mean target encoding)\n",
    "    # Refer: http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-munging/target-encoding.html\n",
    "    # Refer: https://towardsdatascience.com/why-you-should-try-mean-encoding-17057262cd0\n",
    "    elif option == 'ratio':\n",
    "        # For target encoding, we just use 70% of train data to avoid overfitting (otherwise, test AUC drops significantly)\n",
    "        total_cnt = df.sample(frac=0.7).groupby([feature])['illicit']\n",
    "        # prob_illicit = total_cnt.mean()  # Simple mean\n",
    "        adj_prob_illicit = total_cnt.sum() / (total_cnt.count()+adj)  # Smoothed mean\n",
    "        return adj_prob_illicit.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.4. \"tag_risky_profiles\" function [(link to practice)](#practice4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is to generate risk-profiling tags of the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_risky_profiles(df: pd.DataFrame, \n",
    "                       profile: str, \n",
    "                       profiles: list or dict, \n",
    "                       option: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    dtype df: dataframe\n",
    "    dtype profile: str\n",
    "    dtype profiles: list(option='topk') or dictionary(option='ratio')\n",
    "    dtype option: str ('topk', 'ratio')\n",
    "    rtype: dataframe\n",
    "    \n",
    "    The option topk is usually better than the ratio because of overfitting.\n",
    "    \"\"\"\n",
    "    # Top-k suspicious item flagging\n",
    "    if option == 'topk':\n",
    "        d = defaultdict(int) # return 0 for not-defined keys\n",
    "        for id in profiles:\n",
    "            d[id] = 1\n",
    "    #     print(list(islice(d.items(), 10)))  # For debugging\n",
    "        df.loc[:, 'RiskH.'+profile] = df[profile].apply(lambda x: d[x])\n",
    "    \n",
    "    # Illicit-ratio encoding\n",
    "    elif option == 'ratio':\n",
    "        overall_ratio_train = np.mean(train.illicit) # When scripting, saving it as a class variable is clearer.\n",
    "        df.loc[:, 'RiskH.'+profile] = df[profile].apply(lambda x: profiles.get(x,overall_ratio_train))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  1.4. Preprocess data with pre-defined functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset settings\n",
    "data_length = df.shape[0]\n",
    "train_ratio = 0.8\n",
    "valid_ratio = 0.9\n",
    "train_length = int(data_length*train_ratio)\n",
    "valid_length = int(data_length*valid_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train/valid/test set\n",
    "#df = df.sort_values('SGD.DATE')\n",
    "train = df.iloc[:train_length,:]\n",
    "valid = df.iloc[train_length:valid_length,:]\n",
    "test = df.iloc[valid_length:,:]\n",
    "#train = df[df[\"SGD.DATE\"] < \"2015-12-01\"]\n",
    "#valid = df[(df[\"SGD.DATE\"] >= \"2015-12-01\") & (df[\"SGD.DATE\"] < \"2016-01-01\")]\n",
    "#test = df[df[\"SGD.DATE\"] >= \"2016-01-01\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((591630, 14), (73954, 14), (73954, 14))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, valid.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['year', 'month', 'day', 'OFFICE', 'IMPORTER.TIN', 'TARIFF.CODE',\n",
       "       'DECLARANT.CODE', 'ORIGIN.CODE', 'CIF', 'QUANTITY', 'GROSS.WEIGHT',\n",
       "       'TOTAL.TAXES', 'RAISED_TAX_AMOUNT', 'illicit'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save label data\n",
    "#train_reg_label = train['RAISED_TAX_AMOUNT'].values\n",
    "#valid_reg_label = valid['RAISED_TAX_AMOUNT'].values\n",
    "#test_reg_label = test['RAISED_TAX_AMOUNT'].values\n",
    "train_cls_label = train[\"illicit\"].values\n",
    "valid_cls_label = valid[\"illicit\"].values\n",
    "test_cls_label = test[\"illicit\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run preprocessing\n",
    "train = preprocess(train)\n",
    "valid = preprocess(valid)\n",
    "test = preprocess(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['year', 'month', 'day', 'OFFICE', 'IMPORTER.TIN', 'TARIFF.CODE',\n",
       "       'DECLARANT.CODE', 'ORIGIN.CODE', 'CIF', 'QUANTITY', 'GROSS.WEIGHT',\n",
       "       'TOTAL.TAXES', 'RAISED_TAX_AMOUNT', 'illicit', 'Unitprice',\n",
       "       'WUnitprice', 'TaxRatio', 'TaxUnitquantity', 'HS6', 'HS4', 'HS2',\n",
       "       'HS6&ORIGIN.CODE', 'OFFICE&IMPORTER.TIN', 'OFFICE&HS6',\n",
       "       'OFFICE&ORIGIN.CODE'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a few more risky profiles\n",
    "risk_profiles = {}\n",
    "profile_candidates = ['IMPORTER.TIN',\n",
    "                      'ORIGIN.CODE', #'LAST_DEPARTURE.CODE', 'CONTRACT_PARTY.CODE',\n",
    "                      'TARIFF.CODE', 'QUANTITY', 'HS6', 'HS4', 'HS2', 'OFFICE'] + [col for col in train.columns if '&' in col]\n",
    "\n",
    "for profile in profile_candidates:\n",
    "    option = 'topk'\n",
    "    risk_profiles[profile] = find_risk_profile(train, profile, 0.1, 10, option=option)\n",
    "    train = tag_risky_profiles(train, profile, risk_profiles[profile], option=option)\n",
    "    valid = tag_risky_profiles(valid, profile, risk_profiles[profile], option=option)\n",
    "    test = tag_risky_profiles(test, profile, risk_profiles[profile], option=option)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['year', 'month', 'day', 'OFFICE', 'IMPORTER.TIN', 'TARIFF.CODE',\n",
       "       'DECLARANT.CODE', 'ORIGIN.CODE', 'CIF', 'QUANTITY', 'GROSS.WEIGHT',\n",
       "       'TOTAL.TAXES', 'RAISED_TAX_AMOUNT', 'illicit', 'Unitprice',\n",
       "       'WUnitprice', 'TaxRatio', 'TaxUnitquantity', 'HS6', 'HS4', 'HS2',\n",
       "       'HS6&ORIGIN.CODE', 'OFFICE&IMPORTER.TIN', 'OFFICE&HS6',\n",
       "       'OFFICE&ORIGIN.CODE', 'RiskH.IMPORTER.TIN', 'RiskH.ORIGIN.CODE',\n",
       "       'RiskH.TARIFF.CODE', 'RiskH.QUANTITY', 'RiskH.HS6', 'RiskH.HS4',\n",
       "       'RiskH.HS2', 'RiskH.OFFICE', 'RiskH.HS6&ORIGIN.CODE',\n",
       "       'RiskH.OFFICE&IMPORTER.TIN', 'RiskH.OFFICE&HS6',\n",
       "       'RiskH.OFFICE&ORIGIN.CODE'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features to use in a classifier\n",
    "column_to_use = ['CIF', 'TOTAL.TAXES', 'GROSS.WEIGHT', 'QUANTITY',  \n",
    "                 'Unitprice', 'WUnitprice', 'TaxRatio', 'TaxUnitquantity', 'TARIFF.CODE', \n",
    "                 'HS6', 'HS4', 'HS2', 'year', 'month', 'day'] + [col for col in train.columns if 'RiskH' in col] \n",
    "\n",
    "# Extract only numeric values from data to be fed into models\n",
    "X_train = train[column_to_use].values\n",
    "X_valid = valid[column_to_use].values\n",
    "X_test = test[column_to_use].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute nan\n",
    "X_train = np.nan_to_num(X_train, 0)\n",
    "X_valid = np.nan_to_num(X_valid, 0)\n",
    "X_test = np.nan_to_num(X_test, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking data size...\n",
      "(591630, 27) (591630,)\n",
      "(73954, 27) (73954,)\n",
      "(73954, 27) (73954,)\n"
     ]
    }
   ],
   "source": [
    "# make sure the data size are correct\n",
    "print(\"Checking data size...\")\n",
    "print(X_train.shape, train_cls_label.shape) #, train_reg_label.shape\n",
    "print(X_valid.shape, valid_cls_label.shape) #, valid_reg_label.shape\n",
    "print(X_test.shape, test_cls_label.shape) #, test_reg_label.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5. Save all the preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store all data in a dictionary\n",
    "all_data = {\"raw\":{\"train\":train,\"valid\":valid,\"test\":test},\n",
    " \"xgboost_data\":{\"train_x\":X_train,\"train_y\":train_cls_label,\\\n",
    "                 \"valid_x\":X_valid,\"valid_y\":valid_cls_label,\\\n",
    "                 \"test_x\":X_test,\"test_y\":test_cls_label}} \n",
    "# ,\"revenue\":{\"train\":train_reg_label,\"valid\":valid_reg_label,\"test\":test_reg_label}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_x': array([[1.07271419e+04, 6.18947813e+02, 1.10994847e+04, ...,\n",
       "         1.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "        [5.69830285e+02, 3.08607552e+02, 3.11827379e+03, ...,\n",
       "         1.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "        [8.47171727e+03, 1.34961157e+03, 1.14835699e+04, ...,\n",
       "         1.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "        ...,\n",
       "        [4.89809659e+02, 3.48032195e+03, 4.36219283e+03, ...,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "        [1.77793699e+04, 7.06077902e+02, 1.27358157e+04, ...,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "        [3.16972126e+02, 3.47305067e+02, 3.22839432e+04, ...,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00]]),\n",
       " 'train_y': array([0, 0, 1, ..., 0, 0, 0], dtype=int64),\n",
       " 'valid_x': array([[9.25471676e+02, 6.15081877e+02, 1.11021313e+04, ...,\n",
       "         1.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "        [1.17485354e+04, 2.91758385e+03, 9.06236641e+02, ...,\n",
       "         1.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "        [1.32448939e+03, 2.64974944e+03, 1.54957535e+04, ...,\n",
       "         1.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "        ...,\n",
       "        [2.38086894e+03, 4.39179325e+02, 6.27167557e+03, ...,\n",
       "         0.00000000e+00, 0.00000000e+00, 1.00000000e+00],\n",
       "        [1.14484768e+04, 6.66291756e+02, 9.07154820e+02, ...,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "        [1.67561028e+04, 4.82035145e+03, 4.95136262e+02, ...,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00]]),\n",
       " 'valid_y': array([0, 0, 0, ..., 0, 1, 0], dtype=int64),\n",
       " 'test_x': array([[1.56597475e+04, 2.88540834e+03, 1.15185949e+04, ...,\n",
       "         1.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "        [7.33723088e+02, 6.20522639e+03, 1.50682885e+04, ...,\n",
       "         1.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "        [7.54307094e+02, 3.29979985e+02, 1.21572620e+03, ...,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "        ...,\n",
       "        [1.38144263e+03, 1.95792678e+02, 3.51765951e+04, ...,\n",
       "         1.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "        [7.80038991e+03, 1.70877871e+03, 9.92750051e+02, ...,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "        [9.41242517e+02, 3.60172274e+02, 1.41440508e+03, ...,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00]]),\n",
       " 'test_y': array([1, 0, 0, ..., 1, 0, 0], dtype=int64)}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data['xgboost_data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking label distribution\n",
      "Training: 0.19764127644758256\n",
      "Validation: 0.19775200829230372\n",
      "Testing: 0.1970153119031433\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "print(\"Checking label distribution\")\n",
    "cnt = Counter(train_cls_label)\n",
    "print(\"Training:\",cnt[1]/cnt[0])\n",
    "cnt = Counter(valid_cls_label)\n",
    "print(\"Validation:\",cnt[1]/cnt[0])\n",
    "cnt = Counter(test_cls_label)\n",
    "print(\"Testing:\",cnt[1]/cnt[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle a variable to a file\n",
    "# reference for pickle: https://www.datacamp.com/community/tutorials/pickle-python-tutorial\n",
    "file = open('./processed_data.pickle', 'wb')\n",
    "pickle.dump(all_data, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2. XGBoost model <a id='part2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Set environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install xgboost -> https://stackoverflow.com/questions/55332943/importerror-no-module-named-xgboost\n",
    "# Install pytorch -> https://pytorch.org/get-started/locally/#mac-anaconda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import pickle\n",
    "import copy\n",
    "import os \n",
    "from xgboost import XGBClassifier\n",
    "#(toberemoved)from utils import find_best_threshold,process_leaf_idx,stratify_sample\n",
    "from sklearn.metrics import f1_score,roc_auc_score\n",
    "import torch\n",
    "import torch.utils.data as Data\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Load the preprocessed data in Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['raw', 'xgboost_data'])\n",
      "Finish loading data...\n"
     ]
    }
   ],
   "source": [
    "# load preprocessed data\n",
    "# with open(\"./processed_data_13-01-01.pickle\",\"rb\") as f :\n",
    "#     processed_data = pickle.load(f)\n",
    "with open(\"./processed_data.pickle\",\"rb\") as f :\n",
    "    processed_data = pickle.load(f)\n",
    "print(processed_data.keys())\n",
    "print(\"Finish loading data...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into train-, valid- and test-set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/test data \n",
    "train = processed_data[\"raw\"][\"train\"]\n",
    "valid = processed_data[\"raw\"][\"valid\"]\n",
    "test = processed_data[\"raw\"][\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split labels into train-, valid- and test-set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Revenue data for regression target \n",
    "#revenue_train = processed_data[\"revenue\"][\"train\"]\n",
    "#revenue_valid = processed_data[\"revenue\"][\"valid\"]\n",
    "#revenue_test = processed_data[\"revenue\"][\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take logged values of revenue, and normalize them. --> getting more densed distribution/minimizing outliers' impacts.  \n",
    "As we assume no information on valid-data and test-data, they are normalized with the information of train-data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize revenue by f(x) = log(x+1)/max(xi)\n",
    "#norm_revenue_train = np.log(revenue_train+1)\n",
    "#norm_revenue_valid = np.log(revenue_valid+1)\n",
    "#norm_revenue_test = np.log(revenue_test+1) \n",
    "#global_max = max(norm_revenue_train) \n",
    "#norm_revenue_train = norm_revenue_train/global_max\n",
    "#norm_revenue_valid = norm_revenue_valid/global_max\n",
    "#norm_revenue_test = norm_revenue_test/global_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split xgboost data into train-, valid- and test-set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xgboost data \n",
    "xgb_trainx = processed_data[\"xgboost_data\"][\"train_x\"]\n",
    "xgb_trainy = processed_data[\"xgboost_data\"][\"train_y\"]\n",
    "xgb_validx = processed_data[\"xgboost_data\"][\"valid_x\"]\n",
    "xgb_validy = processed_data[\"xgboost_data\"][\"valid_y\"]\n",
    "xgb_testx = processed_data[\"xgboost_data\"][\"test_x\"]\n",
    "xgb_testy = processed_data[\"xgboost_data\"][\"test_y\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, ..., 1, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_testy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Define functions to be used\n",
    "### 2.3.1. \"find_best_threshod\" function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_threshold(model,x_list,y_test,best_thresh = None):\n",
    "    '''\n",
    "    This function is to find the best threshold to determine \"to inspect\" or \"not\".\n",
    "    We assume that we inspect only the imports where predicted probability of fraud is above the threshold.\n",
    "    The input arguments are;\n",
    "    - dtype model: scikit-learn classifier model\n",
    "    - dtype x_list: list or array of features (data)\n",
    "    - dtype y_test: array of true labels \n",
    "    '''\n",
    "    # Predict probability of fraud of each import.\n",
    "    y_pred_prob = model.predict_proba(x_list)[:,1]\n",
    "    # Set threshold range as [0.1, 0.2, ..., 0.5]. \n",
    "    threshold_list = np.arange(0.1,0.6,0.1)\n",
    "    # Set an initial value of best threshold.\n",
    "    best_f1 = 0\n",
    "    # if best_thresh is set as \"None\", this function is to find the best_thresh as well as best_f1 \n",
    "    if best_thresh ==None:\n",
    "        for th in threshold_list:\n",
    "            y_pred_label = (y_pred_prob > th)*1 \n",
    "            f_score = f1_score(y_test,y_pred_label)\n",
    "            if f_score > best_f1:\n",
    "                best_f1 = f_score\n",
    "                best_thresh = th \n",
    "        return best_thresh, best_f1\n",
    "    # if best_thresh is set as a certain number, this function is to calculate its f1 score.\n",
    "    else:\n",
    "        y_pred_label = (y_pred_prob > best_thresh)*1 \n",
    "        best_f1 = f1_score(y_test,y_pred_label)\n",
    "    print(\"F1-scre equals to:%.4f\"%(best_f1))\n",
    "    return best_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. Deploy a XGBoost model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training xgboost model...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
       "              learning_rate=0.1, max_delta_step=0, max_depth=4,\n",
       "              min_child_weight=1, missing=None, n_estimators=100, n_jobs=-1,\n",
       "              nthread=None, objective='binary:logistic', random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "              silent=None, subsample=1, verbosity=1)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Training xgboost model...\")\n",
    "# Convert nparray to pd.DataFrame (why? not necessary)\n",
    "columns = column_to_use\n",
    "xgb_trainx = pd.DataFrame(xgb_trainx,columns=columns)\n",
    "xgb_validx = pd.DataFrame(xgb_validx,columns=columns)\n",
    "xgb_testx = pd.DataFrame(xgb_testx,columns=columns)\n",
    "# Initiate the model\n",
    "xgb_clf = XGBClassifier(n_estimators=100, max_depth=4,n_jobs=-1)\n",
    "# Train/fit the model\n",
    "xgb_clf.fit(xgb_trainx, xgb_trainy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the first tree out of 100 trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install graphviz -> https://anaconda.org/conda-forge/python-graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\r\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n",
       "<!-- Generated by graphviz version 2.38.0 (20140413.2041)\r\n",
       " -->\r\n",
       "<!-- Title: %3 Pages: 1 -->\r\n",
       "<svg width=\"2813pt\" height=\"392pt\"\r\n",
       " viewBox=\"0.00 0.00 2813.29 392.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 388)\">\r\n",
       "<title>%3</title>\r\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-388 2809.29,-388 2809.29,4 -4,4\"/>\r\n",
       "<!-- 0 -->\r\n",
       "<g id=\"node1\" class=\"node\"><title>0</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"1509.99\" cy=\"-366\" rx=\"108.581\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1509.99\" y=\"-362.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">RiskH.OFFICE&amp;HS6&lt;0.5</text>\r\n",
       "</g>\r\n",
       "<!-- 1 -->\r\n",
       "<g id=\"node2\" class=\"node\"><title>1</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"1126.99\" cy=\"-279\" rx=\"116.979\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1126.99\" y=\"-275.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">RiskH.IMPORTER.TIN&lt;0.5</text>\r\n",
       "</g>\r\n",
       "<!-- 0&#45;&gt;1 -->\r\n",
       "<g id=\"edge1\" class=\"edge\"><title>0&#45;&gt;1</title>\r\n",
       "<path fill=\"none\" stroke=\"#0000ff\" d=\"M1447.61,-351.154C1379.99,-336.148 1272.17,-312.22 1200.5,-296.313\"/>\r\n",
       "<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"1201.15,-292.873 1190.63,-294.123 1199.63,-299.707 1201.15,-292.873\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1376.49\" y=\"-318.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">yes, missing</text>\r\n",
       "</g>\r\n",
       "<!-- 2 -->\r\n",
       "<g id=\"node3\" class=\"node\"><title>2</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"1816.99\" cy=\"-279\" rx=\"116.979\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1816.99\" y=\"-275.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">RiskH.IMPORTER.TIN&lt;0.5</text>\r\n",
       "</g>\r\n",
       "<!-- 0&#45;&gt;2 -->\r\n",
       "<g id=\"edge2\" class=\"edge\"><title>0&#45;&gt;2</title>\r\n",
       "<path fill=\"none\" stroke=\"#ff0000\" d=\"M1563.12,-350.291C1615.81,-335.701 1696.62,-313.329 1752.86,-297.757\"/>\r\n",
       "<polygon fill=\"#ff0000\" stroke=\"#ff0000\" points=\"1754.01,-301.07 1762.71,-295.029 1752.14,-294.324 1754.01,-301.07\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1689.49\" y=\"-318.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">no</text>\r\n",
       "</g>\r\n",
       "<!-- 3 -->\r\n",
       "<g id=\"node4\" class=\"node\"><title>3</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"514.993\" cy=\"-192\" rx=\"135.676\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"514.993\" y=\"-188.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">RiskH.HS6&amp;ORIGIN.CODE&lt;0.5</text>\r\n",
       "</g>\r\n",
       "<!-- 1&#45;&gt;3 -->\r\n",
       "<g id=\"edge3\" class=\"edge\"><title>1&#45;&gt;3</title>\r\n",
       "<path fill=\"none\" stroke=\"#0000ff\" d=\"M1043.18,-266.359C931.609,-250.863 734.534,-223.492 615.226,-206.921\"/>\r\n",
       "<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"615.382,-203.409 604.996,-205.5 614.419,-210.343 615.382,-203.409\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"892.493\" y=\"-231.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">yes, missing</text>\r\n",
       "</g>\r\n",
       "<!-- 4 -->\r\n",
       "<g id=\"node5\" class=\"node\"><title>4</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"1126.99\" cy=\"-192\" rx=\"156.772\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1126.99\" y=\"-188.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">RiskH.OFFICE&amp;IMPORTER.TIN&lt;0.5</text>\r\n",
       "</g>\r\n",
       "<!-- 1&#45;&gt;4 -->\r\n",
       "<g id=\"edge4\" class=\"edge\"><title>1&#45;&gt;4</title>\r\n",
       "<path fill=\"none\" stroke=\"#ff0000\" d=\"M1126.99,-260.799C1126.99,-249.163 1126.99,-233.548 1126.99,-220.237\"/>\r\n",
       "<polygon fill=\"#ff0000\" stroke=\"#ff0000\" points=\"1130.49,-220.175 1126.99,-210.175 1123.49,-220.175 1130.49,-220.175\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1134.49\" y=\"-231.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">no</text>\r\n",
       "</g>\r\n",
       "<!-- 5 -->\r\n",
       "<g id=\"node18\" class=\"node\"><title>5</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"1816.99\" cy=\"-192\" rx=\"74.187\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1816.99\" y=\"-188.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">QUANTITY&lt;1.5</text>\r\n",
       "</g>\r\n",
       "<!-- 2&#45;&gt;5 -->\r\n",
       "<g id=\"edge17\" class=\"edge\"><title>2&#45;&gt;5</title>\r\n",
       "<path fill=\"none\" stroke=\"#0000ff\" d=\"M1816.99,-260.799C1816.99,-249.163 1816.99,-233.548 1816.99,-220.237\"/>\r\n",
       "<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"1820.49,-220.175 1816.99,-210.175 1813.49,-220.175 1820.49,-220.175\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1851.49\" y=\"-231.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">yes, missing</text>\r\n",
       "</g>\r\n",
       "<!-- 6 -->\r\n",
       "<g id=\"node19\" class=\"node\"><title>6</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"2236.99\" cy=\"-192\" rx=\"156.772\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"2236.99\" y=\"-188.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">RiskH.OFFICE&amp;IMPORTER.TIN&lt;0.5</text>\r\n",
       "</g>\r\n",
       "<!-- 2&#45;&gt;6 -->\r\n",
       "<g id=\"edge18\" class=\"edge\"><title>2&#45;&gt;6</title>\r\n",
       "<path fill=\"none\" stroke=\"#ff0000\" d=\"M1884.94,-264.249C1958.08,-249.447 2074.49,-225.887 2153.24,-209.95\"/>\r\n",
       "<polygon fill=\"#ff0000\" stroke=\"#ff0000\" points=\"2154.03,-213.362 2163.13,-207.948 2152.64,-206.501 2154.03,-213.362\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"2060.49\" y=\"-231.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">no</text>\r\n",
       "</g>\r\n",
       "<!-- 7 -->\r\n",
       "<g id=\"node6\" class=\"node\"><title>7</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"251.993\" cy=\"-105\" rx=\"74.187\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"251.993\" y=\"-101.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">QUANTITY&lt;1.5</text>\r\n",
       "</g>\r\n",
       "<!-- 3&#45;&gt;7 -->\r\n",
       "<g id=\"edge5\" class=\"edge\"><title>3&#45;&gt;7</title>\r\n",
       "<path fill=\"none\" stroke=\"#0000ff\" d=\"M465.833,-175.112C419.89,-160.263 351.508,-138.163 304.509,-122.973\"/>\r\n",
       "<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"305.412,-119.587 294.821,-119.842 303.26,-126.247 305.412,-119.587\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"434.493\" y=\"-144.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">yes, missing</text>\r\n",
       "</g>\r\n",
       "<!-- 8 -->\r\n",
       "<g id=\"node7\" class=\"node\"><title>8</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"514.993\" cy=\"-105\" rx=\"74.187\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"514.993\" y=\"-101.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">QUANTITY&lt;1.5</text>\r\n",
       "</g>\r\n",
       "<!-- 3&#45;&gt;8 -->\r\n",
       "<g id=\"edge6\" class=\"edge\"><title>3&#45;&gt;8</title>\r\n",
       "<path fill=\"none\" stroke=\"#ff0000\" d=\"M514.993,-173.799C514.993,-162.163 514.993,-146.548 514.993,-133.237\"/>\r\n",
       "<polygon fill=\"#ff0000\" stroke=\"#ff0000\" points=\"518.493,-133.175 514.993,-123.175 511.493,-133.175 518.493,-133.175\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"522.493\" y=\"-144.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">no</text>\r\n",
       "</g>\r\n",
       "<!-- 9 -->\r\n",
       "<g id=\"node12\" class=\"node\"><title>9</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"909.993\" cy=\"-105\" rx=\"135.676\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"909.993\" y=\"-101.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">RiskH.HS6&amp;ORIGIN.CODE&lt;0.5</text>\r\n",
       "</g>\r\n",
       "<!-- 4&#45;&gt;9 -->\r\n",
       "<g id=\"edge11\" class=\"edge\"><title>4&#45;&gt;9</title>\r\n",
       "<path fill=\"none\" stroke=\"#0000ff\" d=\"M1085.15,-174.611C1049.62,-160.691 998.502,-140.669 960.655,-125.844\"/>\r\n",
       "<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"961.924,-122.583 951.336,-122.194 959.371,-129.1 961.924,-122.583\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1066.49\" y=\"-144.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">yes, missing</text>\r\n",
       "</g>\r\n",
       "<!-- 10 -->\r\n",
       "<g id=\"node13\" class=\"node\"><title>10</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"1199.99\" cy=\"-105\" rx=\"135.676\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1199.99\" y=\"-101.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">RiskH.HS6&amp;ORIGIN.CODE&lt;0.5</text>\r\n",
       "</g>\r\n",
       "<!-- 4&#45;&gt;10 -->\r\n",
       "<g id=\"edge12\" class=\"edge\"><title>4&#45;&gt;10</title>\r\n",
       "<path fill=\"none\" stroke=\"#ff0000\" d=\"M1141.77,-173.799C1152.35,-161.471 1166.77,-144.679 1178.62,-130.89\"/>\r\n",
       "<polygon fill=\"#ff0000\" stroke=\"#ff0000\" points=\"1181.38,-133.042 1185.24,-123.175 1176.07,-128.481 1181.38,-133.042\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1175.49\" y=\"-144.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">no</text>\r\n",
       "</g>\r\n",
       "<!-- 15 -->\r\n",
       "<g id=\"node8\" class=\"node\"><title>15</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"77.9931\" cy=\"-18\" rx=\"77.9862\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"77.9931\" y=\"-14.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">leaf=&#45;0.163851395</text>\r\n",
       "</g>\r\n",
       "<!-- 7&#45;&gt;15 -->\r\n",
       "<g id=\"edge7\" class=\"edge\"><title>7&#45;&gt;15</title>\r\n",
       "<path fill=\"none\" stroke=\"#0000ff\" d=\"M219.97,-88.7538C207.204,-82.6478 192.386,-75.5255 178.993,-69 158.981,-59.2494 136.807,-48.2895 118.345,-39.1213\"/>\r\n",
       "<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"119.847,-35.9593 109.334,-34.6423 116.731,-42.2276 119.847,-35.9593\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"213.493\" y=\"-57.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">yes, missing</text>\r\n",
       "</g>\r\n",
       "<!-- 16 -->\r\n",
       "<g id=\"node9\" class=\"node\"><title>16</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"251.993\" cy=\"-18\" rx=\"77.9862\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"251.993\" y=\"-14.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">leaf=&#45;0.133570597</text>\r\n",
       "</g>\r\n",
       "<!-- 7&#45;&gt;16 -->\r\n",
       "<g id=\"edge8\" class=\"edge\"><title>7&#45;&gt;16</title>\r\n",
       "<path fill=\"none\" stroke=\"#ff0000\" d=\"M251.993,-86.799C251.993,-75.1626 251.993,-59.5479 251.993,-46.2368\"/>\r\n",
       "<polygon fill=\"#ff0000\" stroke=\"#ff0000\" points=\"255.493,-46.1754 251.993,-36.1754 248.493,-46.1755 255.493,-46.1754\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"259.493\" y=\"-57.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">no</text>\r\n",
       "</g>\r\n",
       "<!-- 17 -->\r\n",
       "<g id=\"node10\" class=\"node\"><title>17</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"425.993\" cy=\"-18\" rx=\"77.9862\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"425.993\" y=\"-14.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">leaf=&#45;0.128035292</text>\r\n",
       "</g>\r\n",
       "<!-- 8&#45;&gt;17 -->\r\n",
       "<g id=\"edge9\" class=\"edge\"><title>8&#45;&gt;17</title>\r\n",
       "<path fill=\"none\" stroke=\"#0000ff\" d=\"M497.41,-87.2067C484.123,-74.517 465.708,-56.9297 450.905,-42.7925\"/>\r\n",
       "<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"453.189,-40.134 443.54,-35.7584 448.354,-45.1962 453.189,-40.134\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"510.493\" y=\"-57.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">yes, missing</text>\r\n",
       "</g>\r\n",
       "<!-- 18 -->\r\n",
       "<g id=\"node11\" class=\"node\"><title>18</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"604.993\" cy=\"-18\" rx=\"82.5854\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"604.993\" y=\"-14.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">leaf=&#45;0.0667614862</text>\r\n",
       "</g>\r\n",
       "<!-- 8&#45;&gt;18 -->\r\n",
       "<g id=\"edge10\" class=\"edge\"><title>8&#45;&gt;18</title>\r\n",
       "<path fill=\"none\" stroke=\"#ff0000\" d=\"M532.774,-87.2067C546.21,-74.517 564.832,-56.9297 579.801,-42.7925\"/>\r\n",
       "<polygon fill=\"#ff0000\" stroke=\"#ff0000\" points=\"582.382,-45.1692 587.249,-35.7584 577.576,-40.0801 582.382,-45.1692\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"573.493\" y=\"-57.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">no</text>\r\n",
       "</g>\r\n",
       "<!-- 19 -->\r\n",
       "<g id=\"node14\" class=\"node\"><title>19</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"787.993\" cy=\"-18\" rx=\"82.5854\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"787.993\" y=\"-14.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">leaf=&#45;0.0244134646</text>\r\n",
       "</g>\r\n",
       "<!-- 9&#45;&gt;19 -->\r\n",
       "<g id=\"edge13\" class=\"edge\"><title>9&#45;&gt;19</title>\r\n",
       "<path fill=\"none\" stroke=\"#0000ff\" d=\"M885.89,-87.2067C867.054,-74.0831 840.699,-55.7211 820.079,-41.3547\"/>\r\n",
       "<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"821.837,-38.314 811.631,-35.4692 817.835,-44.0575 821.837,-38.314\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"891.493\" y=\"-57.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">yes, missing</text>\r\n",
       "</g>\r\n",
       "<!-- 20 -->\r\n",
       "<g id=\"node15\" class=\"node\"><title>20</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"967.993\" cy=\"-18\" rx=\"79.0865\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"967.993\" y=\"-14.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">leaf=0.0704070404</text>\r\n",
       "</g>\r\n",
       "<!-- 9&#45;&gt;20 -->\r\n",
       "<g id=\"edge14\" class=\"edge\"><title>9&#45;&gt;20</title>\r\n",
       "<path fill=\"none\" stroke=\"#ff0000\" d=\"M921.73,-86.799C930.039,-74.6221 941.321,-58.0889 950.663,-44.3977\"/>\r\n",
       "<polygon fill=\"#ff0000\" stroke=\"#ff0000\" points=\"953.726,-46.1183 956.471,-35.8854 947.944,-42.1728 953.726,-46.1183\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"950.493\" y=\"-57.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">no</text>\r\n",
       "</g>\r\n",
       "<!-- 21 -->\r\n",
       "<g id=\"node16\" class=\"node\"><title>21</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"1140.99\" cy=\"-18\" rx=\"75.2868\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1140.99\" y=\"-14.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">leaf=0.094561331</text>\r\n",
       "</g>\r\n",
       "<!-- 10&#45;&gt;21 -->\r\n",
       "<g id=\"edge15\" class=\"edge\"><title>10&#45;&gt;21</title>\r\n",
       "<path fill=\"none\" stroke=\"#0000ff\" d=\"M1188.05,-86.799C1179.6,-74.6221 1168.13,-58.0889 1158.62,-44.3977\"/>\r\n",
       "<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"1161.29,-42.1046 1152.71,-35.8854 1155.54,-46.0961 1161.29,-42.1046\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1209.49\" y=\"-57.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">yes, missing</text>\r\n",
       "</g>\r\n",
       "<!-- 22 -->\r\n",
       "<g id=\"node17\" class=\"node\"><title>22</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"1309.99\" cy=\"-18\" rx=\"75.2868\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1309.99\" y=\"-14.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">leaf=0.147514924</text>\r\n",
       "</g>\r\n",
       "<!-- 10&#45;&gt;22 -->\r\n",
       "<g id=\"edge16\" class=\"edge\"><title>10&#45;&gt;22</title>\r\n",
       "<path fill=\"none\" stroke=\"#ff0000\" d=\"M1223.6,-87.1361C1231.42,-81.4545 1240.14,-75.0261 1247.99,-69 1259.36,-60.2812 1271.68,-50.4312 1282.32,-41.7931\"/>\r\n",
       "<polygon fill=\"#ff0000\" stroke=\"#ff0000\" points=\"1284.55,-44.4939 1290.09,-35.4626 1280.12,-39.0682 1284.55,-44.4939\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1274.49\" y=\"-57.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">no</text>\r\n",
       "</g>\r\n",
       "<!-- 11 -->\r\n",
       "<g id=\"node20\" class=\"node\"><title>11</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"1634.99\" cy=\"-105\" rx=\"112.38\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1634.99\" y=\"-101.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">RiskH.TARIFF.CODE&lt;0.5</text>\r\n",
       "</g>\r\n",
       "<!-- 5&#45;&gt;11 -->\r\n",
       "<g id=\"edge19\" class=\"edge\"><title>5&#45;&gt;11</title>\r\n",
       "<path fill=\"none\" stroke=\"#0000ff\" d=\"M1784.46,-175.804C1755.05,-162.07 1711.52,-141.739 1679.01,-126.557\"/>\r\n",
       "<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"1680.27,-123.283 1669.73,-122.222 1677.31,-129.625 1680.27,-123.283\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1771.49\" y=\"-144.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">yes, missing</text>\r\n",
       "</g>\r\n",
       "<!-- 12 -->\r\n",
       "<g id=\"node21\" class=\"node\"><title>12</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"1877.99\" cy=\"-105\" rx=\"112.38\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1877.99\" y=\"-101.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">RiskH.TARIFF.CODE&lt;0.5</text>\r\n",
       "</g>\r\n",
       "<!-- 5&#45;&gt;12 -->\r\n",
       "<g id=\"edge20\" class=\"edge\"><title>5&#45;&gt;12</title>\r\n",
       "<path fill=\"none\" stroke=\"#ff0000\" d=\"M1829.04,-174.207C1837.84,-161.946 1849.92,-145.114 1859.88,-131.239\"/>\r\n",
       "<polygon fill=\"#ff0000\" stroke=\"#ff0000\" points=\"1862.77,-133.214 1865.76,-123.049 1857.08,-129.133 1862.77,-133.214\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1859.49\" y=\"-144.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">no</text>\r\n",
       "</g>\r\n",
       "<!-- 13 -->\r\n",
       "<g id=\"node26\" class=\"node\"><title>13</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"2236.99\" cy=\"-105\" rx=\"135.676\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"2236.99\" y=\"-101.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">RiskH.HS6&amp;ORIGIN.CODE&lt;0.5</text>\r\n",
       "</g>\r\n",
       "<!-- 6&#45;&gt;13 -->\r\n",
       "<g id=\"edge25\" class=\"edge\"><title>6&#45;&gt;13</title>\r\n",
       "<path fill=\"none\" stroke=\"#0000ff\" d=\"M2236.99,-173.799C2236.99,-162.163 2236.99,-146.548 2236.99,-133.237\"/>\r\n",
       "<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"2240.49,-133.175 2236.99,-123.175 2233.49,-133.175 2240.49,-133.175\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"2271.49\" y=\"-144.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">yes, missing</text>\r\n",
       "</g>\r\n",
       "<!-- 14 -->\r\n",
       "<g id=\"node27\" class=\"node\"><title>14</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"2552.99\" cy=\"-105\" rx=\"127.277\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"2552.99\" y=\"-101.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">GROSS.WEIGHT&lt;53603.9141</text>\r\n",
       "</g>\r\n",
       "<!-- 6&#45;&gt;14 -->\r\n",
       "<g id=\"edge26\" class=\"edge\"><title>6&#45;&gt;14</title>\r\n",
       "<path fill=\"none\" stroke=\"#ff0000\" d=\"M2295.69,-175.211C2349.82,-160.65 2430.11,-139.054 2486.57,-123.867\"/>\r\n",
       "<polygon fill=\"#ff0000\" stroke=\"#ff0000\" points=\"2487.72,-127.181 2496.47,-121.204 2485.9,-120.422 2487.72,-127.181\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"2421.49\" y=\"-144.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">no</text>\r\n",
       "</g>\r\n",
       "<!-- 23 -->\r\n",
       "<g id=\"node22\" class=\"node\"><title>23</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"1485.99\" cy=\"-18\" rx=\"82.5854\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1485.99\" y=\"-14.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">leaf=&#45;0.0862196386</text>\r\n",
       "</g>\r\n",
       "<!-- 11&#45;&gt;23 -->\r\n",
       "<g id=\"edge21\" class=\"edge\"><title>11&#45;&gt;23</title>\r\n",
       "<path fill=\"none\" stroke=\"#0000ff\" d=\"M1605.91,-87.4094C1582.12,-73.8389 1548.34,-54.5694 1522.7,-39.938\"/>\r\n",
       "<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"1524.41,-36.8881 1513.99,-34.973 1520.94,-42.9683 1524.41,-36.8881\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1604.49\" y=\"-57.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">yes, missing</text>\r\n",
       "</g>\r\n",
       "<!-- 24 -->\r\n",
       "<g id=\"node23\" class=\"node\"><title>24</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"1665.99\" cy=\"-18\" rx=\"79.0865\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1665.99\" y=\"-14.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">leaf=0.0346504562</text>\r\n",
       "</g>\r\n",
       "<!-- 11&#45;&gt;24 -->\r\n",
       "<g id=\"edge22\" class=\"edge\"><title>11&#45;&gt;24</title>\r\n",
       "<path fill=\"none\" stroke=\"#ff0000\" d=\"M1641.27,-86.799C1645.55,-75.0474 1651.32,-59.2383 1656.2,-45.8421\"/>\r\n",
       "<polygon fill=\"#ff0000\" stroke=\"#ff0000\" points=\"1659.59,-46.7694 1659.73,-36.1754 1653.01,-44.3709 1659.59,-46.7694\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1660.49\" y=\"-57.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">no</text>\r\n",
       "</g>\r\n",
       "<!-- 25 -->\r\n",
       "<g id=\"node24\" class=\"node\"><title>25</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"1845.99\" cy=\"-18\" rx=\"82.5854\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1845.99\" y=\"-14.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">leaf=&#45;0.0301711988</text>\r\n",
       "</g>\r\n",
       "<!-- 12&#45;&gt;25 -->\r\n",
       "<g id=\"edge23\" class=\"edge\"><title>12&#45;&gt;25</title>\r\n",
       "<path fill=\"none\" stroke=\"#0000ff\" d=\"M1871.52,-86.799C1867.09,-75.0474 1861.14,-59.2383 1856.1,-45.8421\"/>\r\n",
       "<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"1859.26,-44.301 1852.46,-36.1754 1852.71,-46.7674 1859.26,-44.301\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1898.49\" y=\"-57.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">yes, missing</text>\r\n",
       "</g>\r\n",
       "<!-- 26 -->\r\n",
       "<g id=\"node25\" class=\"node\"><title>26</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"2025.99\" cy=\"-18\" rx=\"79.0865\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"2025.99\" y=\"-14.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">leaf=0.0620431118</text>\r\n",
       "</g>\r\n",
       "<!-- 12&#45;&gt;26 -->\r\n",
       "<g id=\"edge24\" class=\"edge\"><title>12&#45;&gt;26</title>\r\n",
       "<path fill=\"none\" stroke=\"#ff0000\" d=\"M1906.88,-87.4094C1930.41,-73.8982 1963.77,-54.7376 1989.2,-40.1299\"/>\r\n",
       "<polygon fill=\"#ff0000\" stroke=\"#ff0000\" points=\"1991.25,-42.9885 1998.18,-34.973 1987.77,-36.9183 1991.25,-42.9885\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"1968.49\" y=\"-57.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">no</text>\r\n",
       "</g>\r\n",
       "<!-- 27 -->\r\n",
       "<g id=\"node28\" class=\"node\"><title>27</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"2202.99\" cy=\"-18\" rx=\"79.0865\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"2202.99\" y=\"-14.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">leaf=0.0518167429</text>\r\n",
       "</g>\r\n",
       "<!-- 13&#45;&gt;27 -->\r\n",
       "<g id=\"edge27\" class=\"edge\"><title>13&#45;&gt;27</title>\r\n",
       "<path fill=\"none\" stroke=\"#0000ff\" d=\"M2230.11,-86.799C2225.41,-75.0474 2219.09,-59.2383 2213.73,-45.8421\"/>\r\n",
       "<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"2216.83,-44.1603 2209.86,-36.1754 2210.33,-46.7601 2216.83,-44.1603\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"2257.49\" y=\"-57.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">yes, missing</text>\r\n",
       "</g>\r\n",
       "<!-- 28 -->\r\n",
       "<g id=\"node29\" class=\"node\"><title>28</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"2379.99\" cy=\"-18\" rx=\"79.0865\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"2379.99\" y=\"-14.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">leaf=0.0934699103</text>\r\n",
       "</g>\r\n",
       "<!-- 13&#45;&gt;28 -->\r\n",
       "<g id=\"edge28\" class=\"edge\"><title>13&#45;&gt;28</title>\r\n",
       "<path fill=\"none\" stroke=\"#ff0000\" d=\"M2265.21,-87.2738C2274.96,-81.4889 2285.96,-74.9623 2295.99,-69 2311.82,-59.5948 2329.32,-49.1818 2344.23,-40.3043\"/>\r\n",
       "<polygon fill=\"#ff0000\" stroke=\"#ff0000\" points=\"2346.34,-43.1215 2353.14,-34.9979 2342.76,-37.1071 2346.34,-43.1215\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"2327.49\" y=\"-57.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">no</text>\r\n",
       "</g>\r\n",
       "<!-- 29 -->\r\n",
       "<g id=\"node30\" class=\"node\"><title>29</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"2552.99\" cy=\"-18\" rx=\"75.2868\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"2552.99\" y=\"-14.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">leaf=0.160502702</text>\r\n",
       "</g>\r\n",
       "<!-- 14&#45;&gt;29 -->\r\n",
       "<g id=\"edge29\" class=\"edge\"><title>14&#45;&gt;29</title>\r\n",
       "<path fill=\"none\" stroke=\"#0000ff\" d=\"M2552.99,-86.799C2552.99,-75.1626 2552.99,-59.5479 2552.99,-46.2368\"/>\r\n",
       "<polygon fill=\"#0000ff\" stroke=\"#0000ff\" points=\"2556.49,-46.1754 2552.99,-36.1754 2549.49,-46.1755 2556.49,-46.1754\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"2587.49\" y=\"-57.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">yes, missing</text>\r\n",
       "</g>\r\n",
       "<!-- 30 -->\r\n",
       "<g id=\"node31\" class=\"node\"><title>30</title>\r\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"2725.99\" cy=\"-18\" rx=\"79.0865\" ry=\"18\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"2725.99\" y=\"-14.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">leaf=0.0933333337</text>\r\n",
       "</g>\r\n",
       "<!-- 14&#45;&gt;30 -->\r\n",
       "<g id=\"edge30\" class=\"edge\"><title>14&#45;&gt;30</title>\r\n",
       "<path fill=\"none\" stroke=\"#ff0000\" d=\"M2587.58,-87.5593C2599.75,-81.7401 2613.5,-75.1194 2625.99,-69 2645.74,-59.3319 2667.58,-48.4319 2685.81,-39.2823\"/>\r\n",
       "<polygon fill=\"#ff0000\" stroke=\"#ff0000\" points=\"2687.62,-42.2914 2694.98,-34.6731 2684.47,-36.0371 2687.62,-42.2914\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"2664.49\" y=\"-57.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">no</text>\r\n",
       "</g>\r\n",
       "</g>\r\n",
       "</svg>\r\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x27a02f87ac8>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "xgb.to_graphviz(booster = xgb_clf, num_trees=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------Evaluating xgboost model------\n",
      "F1-scre equals to:0.4017\n",
      "AUC = 0.7112, F1-score = 0.4017\n"
     ]
    }
   ],
   "source": [
    "# evaluate xgboost model\n",
    "print(\"------Evaluating xgboost model------\")\n",
    "# Predict\n",
    "test_pred = xgb_clf.predict_proba(xgb_testx)[:,1]\n",
    "# Calculate auc\n",
    "xgb_auc = roc_auc_score(xgb_testy, test_pred)\n",
    "# Find the best threshold\n",
    "xgb_threshold,_ = find_best_threshold(xgb_clf, xgb_validx, xgb_validy)\n",
    "# Calculate the best f1 score\n",
    "xgb_f1 = find_best_threshold(xgb_clf, xgb_testx, xgb_testy,best_thresh=xgb_threshold)\n",
    "print(\"AUC = %.4f, F1-score = %.4f\" % (xgb_auc, xgb_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3. DATE model (XGB + Neural Networks + Attention) <a id='part3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Set environmenst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import pickle\n",
    "import warnings\n",
    "import time \n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install ranger -> https://github.com/lessw2020/Ranger-Deep-Learning-Optimizer\n",
    "# You may restart your machine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from model.AttTreeEmbedding import Attention, DATE\n",
    "from ranger import Ranger\n",
    "#from utils import torch_threshold, fgsm_attack, metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Preprocessing data: Integer-encoding of IMPORTER.TIN and TARIFF.CODE for attention mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user & item information \n",
    "train_raw_importers = train['IMPORTER.TIN'].values\n",
    "train_raw_items = train['TARIFF.CODE'].values\n",
    "valid_raw_importers = valid['IMPORTER.TIN'].values\n",
    "valid_raw_items = valid['TARIFF.CODE'].values\n",
    "test_raw_importers = test['IMPORTER.TIN']\n",
    "test_raw_items = test['TARIFF.CODE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need padding for unseen user or item \n",
    "importer_set = set(train_raw_importers)\n",
    "item_set = set(train_raw_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember to +1 for zero padding \n",
    "importer_mapping = {v:i+1 for i,v in enumerate(importer_set)} \n",
    "hs6_mapping = {v:i+1 for i,v in enumerate(item_set)}\n",
    "importer_size = len(importer_mapping) + 1\n",
    "item_size = len(hs6_mapping) + 1\n",
    "# label-encoding\n",
    "train_importers = [importer_mapping[x] for x in train_raw_importers]\n",
    "train_items = [hs6_mapping[x] for x in train_raw_items]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for test data, we use padding_idx=0 for unseen data\n",
    "# use dic.get(key,deafault) to handle unseen\n",
    "valid_importers = [importer_mapping.get(x,0) for x in valid_raw_importers]\n",
    "valid_items = [hs6_mapping.get(x,0) for x in valid_raw_items]\n",
    "test_importers = [importer_mapping.get(x,0) for x in test_raw_importers] \n",
    "test_items = [hs6_mapping.get(x,0) for x in test_raw_items]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Define functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1. \"process_leaf_idx\" function [(link to practice)](#practice5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_leaf_idx(X_leaves): \n",
    "    '''\n",
    "    This function is to convert the output of XGBoost model to the input of DATE model.\n",
    "    For an individual import, the output of XGBoost model is a list of leaf index of multiple trees.\n",
    "    eg. [1, 1, 10, 9, 30, 30, 32, ... ]\n",
    "    How to distinguish \"node 1\" of the first tree from \"node 1\" of the second tree?\n",
    "    How to distinguish \"node 30\" of the fifth tree from \"node 30\" of the sixth tree?\n",
    "    This function is to assign unique index to every leaf node in all the trees. \n",
    "    This function returns;\n",
    "    - lists of unique leaf index;\n",
    "    - total number of unique leaf nodes; and\n",
    "    - a reference table (dictionary) composed of \"unique leaf index\", \"tree id\", \"(previous) leaf index\". \n",
    "    '''\n",
    "    leaves = X_leaves.copy()\n",
    "    new_leaf_index = dict() # dictionary to store leaf index\n",
    "    total_leaves = 0\n",
    "    for c in range(X_leaves.shape[1]): # iterate for each column (ie. 100 trees)\n",
    "        column = X_leaves[:,c]\n",
    "        unique_vals = list(sorted(set(column)))\n",
    "        new_idx = {v:(i+total_leaves) for i,v in enumerate(unique_vals)}\n",
    "        for i,v in enumerate(unique_vals):\n",
    "            leaf_id = i+total_leaves\n",
    "            new_leaf_index[leaf_id] = {c:v}\n",
    "        leaves[:,c] = [new_idx[v] for v in column]\n",
    "        total_leaves += len(unique_vals)\n",
    "        \n",
    "    assert leaves.ravel().max() == total_leaves - 1\n",
    "    return leaves,total_leaves,new_leaf_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2. \"fgsm_attack\" function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fgsm_attack(model, loss, images, labels, eps) :\n",
    "    # images.requires_grad = True\n",
    "    images = Variable(images, requires_grad=True)\n",
    "    outputs = model.module.pred_from_hidden(images)\n",
    "    \n",
    "    model.zero_grad()\n",
    "    cost = loss(outputs, labels)\n",
    "    cost.backward()\n",
    "    attack_images = images + eps * images.grad.sign()\n",
    "    # attack_images = images + eps * F.normalize(images.grad.data, dim=0, p=2)\n",
    "    # attack_images.requires_grad = False\n",
    "    return attack_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.3. \"metrics\" function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(y_prob,xgb_testy,best_thresh=None): #,revenue_test\n",
    "    if best_thresh ==None:\n",
    "        _,overall_f1,auc = torch_threshold(y_prob,xgb_testy,best_thresh)\n",
    "    else:\n",
    "        overall_f1,auc = torch_threshold(y_prob,xgb_testy,best_thresh)\n",
    "     # Seized revenue \n",
    "    # Precision and Recall\n",
    "    pr, re, f = [], [], [] #, rev, []\n",
    "    for i in [99,98,95,90]:\n",
    "        threshold = np.percentile(y_prob, i)\n",
    "        #print(f'Checking top {100-i}% suspicious transactions: {len(y_prob[y_prob > threshold])}')\n",
    "        precision = np.mean(xgb_testy[y_prob > threshold])\n",
    "        recall = sum(xgb_testy[y_prob > threshold])/sum(xgb_testy)\n",
    "        f1 = 2*precision*recall/(precision+recall)\n",
    "        #revenue_recall = sum(revenue_test[y_prob > threshold]) /sum(revenue_test)\n",
    "\n",
    "        # save results\n",
    "        pr.append(precision)\n",
    "        re.append(recall)\n",
    "        f.append(f1)\n",
    "        #rev.append(revenue_recall)\n",
    "        # print(f'Precision: {round(precision, 4)}, Recall: {round(recall, 4)}, Seized Revenue (Recall): {round(revenue_recall, 4)}')\n",
    "    return overall_f1,auc,pr, re, f #, rev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.4. \"torch_threshold\" function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_threshold(y_pred_prob,y_test,best_thresh = None):\n",
    "    '''\n",
    "    This function is to find the best threshold to determine \"to inspect\" or \"not\".\n",
    "    We assume that we inspect only the imports where predicted probability of fraud is above the threshold.\n",
    "    '''\n",
    "    threshold_list = np.arange(0.1,0.6,0.1)\n",
    "    best_f1 = 0\n",
    "    if best_thresh == None:\n",
    "        for th in threshold_list:\n",
    "            y_pred_label = (y_pred_prob > th)*1 \n",
    "            f_score = f1_score(y_test,y_pred_label)\n",
    "            if f_score > best_f1:\n",
    "                best_f1 = f_score\n",
    "                best_thresh = th \n",
    "        return best_thresh, best_f1, roc_auc_score(y_test, y_pred_prob)\n",
    "    else:\n",
    "        y_pred_label = (y_pred_prob > best_thresh)*1 \n",
    "        best_f1 = f1_score(y_test,y_pred_label)\n",
    "        return best_f1, roc_auc_score(y_test, y_pred_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. Identify leaf nodes of individual import from XGB model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import  OneHotEncoder\n",
    "\n",
    "# get leaf index from xgboost model \n",
    "X_train_leaves = xgb_clf.apply(xgb_trainx) #apply: Return the predicted leaf every tree for each sample.\n",
    "X_valid_leaves = xgb_clf.apply(xgb_validx)\n",
    "X_test_leaves = xgb_clf.apply(xgb_testx)\n",
    "train_rows = X_train_leaves.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess\n",
    "train_rows = train.shape[0]\n",
    "valid_rows = valid.shape[0] + train_rows\n",
    "X_leaves = np.concatenate((X_train_leaves, X_valid_leaves, X_test_leaves), axis=0) # make sure the dimensionality\n",
    "transformed_leaves, leaf_num, new_leaf_index = process_leaf_idx(X_leaves)\n",
    "train_leaves, valid_leaves, test_leaves = transformed_leaves[:train_rows],\\\n",
    "                                          transformed_leaves[train_rows:valid_rows],\\\n",
    "                                          transformed_leaves[valid_rows:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5. Convert data to tensor\n",
    "Tensor is a collection of numbers with specific shape (dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to torch type\n",
    "train_leaves = torch.tensor(train_leaves).long()\n",
    "train_user = torch.tensor(train_importers).long()\n",
    "train_item = torch.tensor(train_items).long()\n",
    "\n",
    "valid_leaves = torch.tensor(valid_leaves).long()\n",
    "valid_user = torch.tensor(valid_importers).long()\n",
    "valid_item = torch.tensor(valid_items).long()\n",
    "\n",
    "test_leaves = torch.tensor(test_leaves).long()\n",
    "test_user = torch.tensor(test_importers).long()\n",
    "test_item = torch.tensor(test_items).long()\n",
    "\n",
    "# cls data\n",
    "train_label_cls = torch.tensor(xgb_trainy).float()\n",
    "valid_label_cls = torch.tensor(xgb_validy).float()\n",
    "test_label_cls = torch.tensor(xgb_testy).float()\n",
    "\n",
    "# revenue data \n",
    "#train_label_reg = torch.tensor(norm_revenue_train).float()\n",
    "#valid_label_reg = torch.tensor(revenue_valid).float()\n",
    "#test_label_reg = torch.tensor(revenue_test).float()\n",
    "\n",
    "# create dataloader \n",
    "\n",
    "train_dataset = Data.TensorDataset(train_leaves,train_user,train_item,train_label_cls)#,train_label_reg\n",
    "valid_dataset = Data.TensorDataset(valid_leaves,valid_user,valid_item,valid_label_cls)#,valid_label_reg\n",
    "test_dataset = Data.TensorDataset(test_leaves,test_user,test_item,test_label_cls)#,test_label_reg\n",
    "\n",
    "\n",
    "\n",
    "data4embedding = {\"train_dataset\":train_dataset,\"valid_dataset\":valid_dataset,\"test_dataset\":test_dataset,\\\n",
    "                  \"leaf_num\":leaf_num,\"importer_num\":importer_size,\"item_size\":item_size}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6. Save and load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6.1. Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"torch_data_test.pickle\", 'wb') as f:\n",
    "    pickle.dump(data4embedding, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open(\"leaf_index.pickle\", \"wb\") as f:\n",
    "    pickle.dump(new_leaf_index, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6.2. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load torch dataset \n",
    "with open(\"torch_data_test.pickle\",\"rb\") as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get torch dataset \n",
    "train_dataset = data[\"train_dataset\"]\n",
    "valid_dataset = data[\"valid_dataset\"]\n",
    "test_dataset = data[\"test_dataset\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataloader\n",
    "batch_size = 256\n",
    "train_loader = Data.DataLoader(\n",
    "    dataset=train_dataset,     \n",
    "    batch_size=batch_size,      \n",
    "    shuffle=True,               \n",
    ")\n",
    "valid_loader = Data.DataLoader(\n",
    "    dataset=valid_dataset,     \n",
    "    batch_size=batch_size,      \n",
    "    shuffle=False,               \n",
    ")\n",
    "test_loader = Data.DataLoader(\n",
    "    dataset=test_dataset,     \n",
    "    batch_size=batch_size,      \n",
    "    shuffle=False,               \n",
    ")\n",
    "\n",
    "# parameters for model \n",
    "leaf_num = data[\"leaf_num\"]\n",
    "importer_size = data[\"importer_num\"]\n",
    "item_size = data[\"item_size\"]\n",
    "\n",
    "# global variables\n",
    "xgb_validy = valid_loader.dataset.tensors[-1].detach().numpy() # wow finally.... \n",
    "xgb_testy = test_loader.dataset.tensors[-1].detach().numpy()\n",
    "#revenue_valid = valid_loader.dataset.tensors[-1].detach().numpy()\n",
    "#revenue_test = test_loader.dataset.tensors[-1].detach().numpy()\n",
    "\n",
    "# model information\n",
    "curr_time = str(time.time())\n",
    "model_name = \"DATE\"\n",
    "model_path = \"./saved_models/%s%s.pkl\" % (model_name,curr_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7. Model construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install torch_multi_head_attention -> https://github.com/CyberZHG/torch-multi-head-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as Data\n",
    "import numpy as np \n",
    "from torch_multi_head_attention import MultiHeadAttention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[link to practice Mish()](#practice6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mish(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Mish,self).__init__()\n",
    "    def forward(self, x):\n",
    "        return x *( torch.tanh(F.softplus(x))) #softplus???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[link to practice FusionAttention()](#practice7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FusionAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self,dim):\n",
    "        super(FusionAttention, self).__init__()\n",
    "        self.attention_matrix = nn.Linear(dim, dim) # nn.Linear(size of input, size of output)\n",
    "        self.project_weight = nn.Linear(dim,1)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        query_project = self.attention_matrix(inputs) # (b,t,d) -> (b,t,d2)\n",
    "        query_project = F.leaky_relu(query_project)\n",
    "        project_value = self.project_weight(query_project) # (b,t,h) -> (b,t,1)\n",
    "        attention_weight = torch.softmax(project_value, dim=1) # Normalize and calculate weights (b,t,1)\n",
    "        attention_vec = inputs * attention_weight\n",
    "        attention_vec = torch.sum(attention_vec,dim=1)\n",
    "        return attention_vec, attention_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[link to practice Attention()](#practice8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    \n",
    "    def __init__(self,dim,hidden,aggregate=\"sum\"):\n",
    "        super(Attention, self).__init__()\n",
    "        self.attention_matrix = nn.Linear(dim, hidden)\n",
    "        self.project_weight = nn.Linear(hidden*2,hidden)\n",
    "        self.h = nn.Parameter(torch.rand(hidden,1))\n",
    "        self.agg_type = aggregate\n",
    "        \n",
    "    def forward(self, query, key): # query: 256 X 16, # key: 256 X 100 X 16, # assume key==value\n",
    "        dim = query.size(-1) # 16 (n_embedding_dimension)\n",
    "        batch = key.size(0) # 256 (batch_size = n_observation in a batch)\n",
    "        time_step = key.size(1) # 100 (n_trees from xgboot model)\n",
    "        \n",
    "        # concate input query and key \n",
    "        query = query.view(batch,1,dim) # view = reshape: (256X16) -> (256X1X16)\n",
    "        query = query.expand(batch,time_step,-1) # expand to the same dimension: (256X1X16) -> (256X100X16)\n",
    "        cat_vector = torch.cat((query,key),dim=-1) # (256X100X32)\n",
    "        \n",
    "        # project to single value\n",
    "        project_vector = self.project_weight(cat_vector) \n",
    "        project_vector = torch.relu(project_vector)\n",
    "        attention_alpha = torch.matmul(project_vector,self.h)\n",
    "        attention_weight = torch.softmax(attention_alpha, dim=1) # Normalize and calculate weights (b,t,1)\n",
    "        attention_vec = key * attention_weight\n",
    "        \n",
    "        # aggregate leaves\n",
    "        if self.agg_type == \"max\":\n",
    "            attention_vec,_ = torch.max(attention_vec,dim=1)\n",
    "        elif self.agg_type ==\"mean\":\n",
    "            attention_vec = torch.mean(attention_vec,dim=1)\n",
    "        elif self.agg_type ==\"sum\":\n",
    "            attention_vec = torch.sum(attention_vec,dim=1)\n",
    "        return attention_vec, attention_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[link to practice DATE()](#practice9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DATE(nn.Module):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 max_leaf,importer_size,item_size,dim,\n",
    "                 head_num=4,fusion_type=\"concat\",act=\"relu\",device=\"cpu\",use_self=True,agg_type=\"sum\"):\n",
    "        super(DATE, self).__init__()\n",
    "        self.d = dim\n",
    "        self.device = device\n",
    "        if act == \"relu\":\n",
    "            self.act = nn.LeakyReLU()\n",
    "        elif act == \"mish\":\n",
    "            self.act = Mish() \n",
    "        self.fusion_type = fusion_type\n",
    "        self.use_self = use_self\n",
    "\n",
    "        # embedding layers \n",
    "        self.leaf_embedding = nn.Embedding(max_leaf,dim)\n",
    "        self.user_embedding = nn.Embedding(importer_size,dim,padding_idx=0)\n",
    "        self.user_embedding.weight.data[0] = torch.zeros(dim) # unseen data? initial value?\n",
    "        self.item_embedding = nn.Embedding(item_size,dim,padding_idx=0)\n",
    "        self.item_embedding.weight.data[0] = torch.zeros(dim)\n",
    "\n",
    "        # attention layer\n",
    "        self.attention_bolck = Attention(dim,dim,agg_type).to(device)\n",
    "        self.self_att = MultiHeadAttention(dim,head_num).to(device)\n",
    "        self.fusion_att = FusionAttention(dim)\n",
    "\n",
    "        # Hidden & output layer\n",
    "        self.layer_norm = nn.LayerNorm((100,dim))\n",
    "        self.fussionlayer = nn.Linear(dim*3,dim)\n",
    "        self.hidden = nn.Linear(dim,dim)\n",
    "        self.output_cls_layer = nn.Linear(dim,1)\n",
    "        #self.output_reg_layer = nn.Linear(dim,1)\n",
    "    \n",
    "    def forward(self,feature,uid,item_id):\n",
    "        \n",
    "        # Embedding of leaf_id\n",
    "        leaf_vectors = self.leaf_embedding(feature)\n",
    "        \n",
    "        # 1st attention: Multi-Head Self-Attention\n",
    "        # Calculate the weight(importance) of each leaf(cross-feature) based on the correlation with other leafs\n",
    "        if self.use_self:\n",
    "            # Apply multy head attention\n",
    "            leaf_vectors = self.self_att(leaf_vectors,leaf_vectors,leaf_vectors)\n",
    "            # Normalization?\n",
    "            leaf_vectors = self.layer_norm(leaf_vectors)\n",
    "        \n",
    "        # Embedding of importer_id\n",
    "        importer_vector = self.user_embedding(uid)\n",
    "        # Embedding of item_id\n",
    "        item_vector = self.item_embedding(item_id)\n",
    "        # Multiply embeddings of importer_id and item_id\n",
    "        query_vector = importer_vector * item_vector\n",
    "        \n",
    "        # 2nd attention: Attention with leaf_id, importer_id and item_id (all embeddings)\n",
    "        set_vector, self.attention_w = self.attention_bolck(query_vector,leaf_vectors)\n",
    "        \n",
    "        # concat the user, item and tree vectors into a fusion attention\n",
    "        if self.fusion_type == \"concat\":\n",
    "            fusion = torch.cat((importer_vector, item_vector, set_vector), dim=-1) # attach as columns\n",
    "            fusion = self.act(self.fussionlayer(fusion))\n",
    "        elif self.fusion_type == \"attention\":\n",
    "            importer_vector=importer_vector.view(-1,1,self.d), \n",
    "            item_vector=item_vector.view(-1,1,self.d), \n",
    "            set_vector=set_vector.view(-1,1,self.d)\n",
    "            fusion = torch.cat((importer_vector, item_vector, set_vector), dim=1) # attach as columns\n",
    "            fusion,_ = self.fusion_att(fusion)\n",
    "        else:\n",
    "            raise \"Fusion type error\"\n",
    "        hidden = self.hidden(fusion)\n",
    "        hidden = self.act(hidden)\n",
    "\n",
    "        # multi-task output \n",
    "        classification_output = torch.sigmoid(self.output_cls_layer(hidden))\n",
    "        #regression_output = torch.relu(self.output_reg_layer(hidden))\n",
    "        return classification_output, hidden #, regression_output\n",
    "\n",
    "    def pred_from_hidden(self,hidden):\n",
    "        classification_output = torch.sigmoid(self.output_cls_layer(hidden))\n",
    "        return classification_output \n",
    "\n",
    "    def eval_on_batch(self,test_loader): # predict test data using batch \n",
    "        final_output = []\n",
    "        cls_loss = []\n",
    "        #reg_loss = []\n",
    "        for batch in test_loader:\n",
    "            batch_feature, batch_user, batch_item, batch_cls = batch #, batch_reg\n",
    "            batch_feature, batch_user, batch_item, batch_cls =  batch_feature.to(self.device), batch_user.to(self.device),\\\n",
    "            batch_item.to(self.device), batch_cls.to(self.device)#, batch_reg.to(self.device)\n",
    "            batch_cls= batch_cls.view(-1,1)#,batch_reg , batch_reg.view(-1,1)\n",
    "            y_pred_prob,_ = self.forward(batch_feature,batch_user,batch_item) #, y_pred_rev\n",
    "\n",
    "            # compute classification loss\n",
    "            cls_losses = nn.BCELoss()(y_pred_prob,batch_cls)\n",
    "            cls_loss.append(cls_losses.item())\n",
    "\n",
    "            # compute regression loss \n",
    "            #reg_losses = nn.MSELoss()(y_pred_rev, batch_reg)\n",
    "            #reg_loss.append(reg_losses.item())\n",
    "\n",
    "            # store predicted probability \n",
    "            y_pred = y_pred_prob.detach().cpu().numpy().tolist()\n",
    "            final_output.extend(y_pred)\n",
    "\n",
    "        print(\"CLS loss: %.4f\"% (np.mean(cls_loss)) ) #, REG loss: %.4f, np.mean(reg_loss)\n",
    "        return np.array(final_output).ravel(), np.mean(cls_loss) #+ np.mean(reg_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.8. Train (hyperparameter + loss function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.8.1. Set hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--save'], dest='save', nargs=None, const=None, default=0, type=<class 'int'>, choices=None, help='save model or not', metavar=None)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parse argument\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--model_name', \n",
    "                        type=str, \n",
    "                        default=\"DATE\", \n",
    "                        help=\"Name of model\",\n",
    "                        )\n",
    "parser.add_argument('--epoch', \n",
    "                        type=int, \n",
    "                        default=5, \n",
    "                        help=\"Number of epochs\",\n",
    "                        )\n",
    "parser.add_argument('--dim', \n",
    "                        type=int, \n",
    "                        default=16, \n",
    "                        help=\"Hidden layer dimension\",\n",
    "                        )\n",
    "parser.add_argument('--lr', \n",
    "                        type=float, \n",
    "                        default=0.005, \n",
    "                        help=\"learning rate\",\n",
    "                        )\n",
    "parser.add_argument('--l2',\n",
    "                        type=float,\n",
    "                        default=0.00,\n",
    "                        help=\"l2 reg\",\n",
    "                        )\n",
    "'''parser.add_argument('--alpha',\n",
    "                        type=float,\n",
    "                        default=10,\n",
    "                        help=\"Regression loss weight\",\n",
    "                        )'''\n",
    "parser.add_argument('--beta', type=float, default=0.00, help=\"Adversarial loss weight\")\n",
    "parser.add_argument('--head_num', type=int, default=4, help=\"Number of heads for self attention\")\n",
    "parser.add_argument('--use_self', type=int, default=1, help=\"Wheter to use self attention\")\n",
    "parser.add_argument('--fusion', type=str, choices=[\"concat\",\"attention\"], default=\"concat\", help=\"Fusion method for final embedding\")\n",
    "parser.add_argument('--agg', type=str, choices=[\"sum\",\"max\",\"mean\"], default=\"sum\", help=\"Aggreate type for leaf embedding\")\n",
    "parser.add_argument('--act', type=str, choices=[\"mish\",\"relu\"], default=\"relu\", help=\"Activation function\")\n",
    "parser.add_argument('--device', type=str, choices=[\"cuda:0\",\"cuda:1\",\"cpu\"], default=\"cuda:0\", help=\"device name for training\")\n",
    "parser.add_argument('--output', type=str, default=\"full.csv\", help=\"Name of output file\")\n",
    "parser.add_argument('--save', type=int, default=0, help=\"save model or not\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parser.parse_args([])\n",
    "# Why an empty list?\n",
    "# Refere to: https://stackoverflow.com/questions/50763033/argparse-in-jupyter-notebook-throws-a-typeerror"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.8.2. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare directory for saving results.\n",
    "import os\n",
    "if not os.path.exists('./saved_models'):\n",
    "    os.makedirs('./saved_models')\n",
    "if not os.path.exists('./results'):\n",
    "    os.makedirs('./results')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args):\n",
    "    # get configs\n",
    "    epochs = 20 #args.epoch\n",
    "    dim = args.dim\n",
    "    lr = args.lr\n",
    "    weight_decay = args.l2\n",
    "    head_num = args.head_num\n",
    "    device = 'cpu' #args.device\n",
    "    act = args.act\n",
    "    fusion = args.fusion\n",
    "    beta = args.beta\n",
    "    #alpha = args.alpha\n",
    "    use_self = args.use_self\n",
    "    agg = args.agg\n",
    "    model = DATE(leaf_num,importer_size,item_size,\\\n",
    "                                    dim,head_num,\\\n",
    "                                    fusion_type=fusion,act=act,device=device,\\\n",
    "                                    use_self=use_self,agg_type=agg,\n",
    "                                    ).to(device)\n",
    "    model = nn.DataParallel(model,device_ids=[0,1])\n",
    "\n",
    "    # initialize parameters\n",
    "    # Fills the input Tensor with values according to the method described in \n",
    "    # Understanding the difficulty of training deep feedforward neural networks - Glorot, X. & Bengio, Y. (2010), \n",
    "    # using a uniform distribution.\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "\n",
    "    # optimizer & loss \n",
    "    optimizer = Ranger(model.parameters(), weight_decay=weight_decay,lr=lr)\n",
    "    cls_loss_func = nn.BCELoss()\n",
    "    #reg_loss_func = nn.MSELoss()\n",
    "\n",
    "    # save best model\n",
    "    global_best_score = 0\n",
    "    model_state = None\n",
    "\n",
    "    # early stop settings \n",
    "    stop_rounds = 3\n",
    "    no_improvement = 0\n",
    "    current_score = None \n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for step, (batch_feature,batch_user,batch_item,batch_cls) in enumerate(train_loader): #,batch_reg\n",
    "            model.train() # prep to train model\n",
    "            batch_feature,batch_user,batch_item,batch_cls = batch_feature.to(device), batch_user.to(device), batch_item.to(device),\\\n",
    "             batch_cls.to(device) #, batch_reg.to(device)\n",
    "            batch_cls = batch_cls.view(-1,1) #,batch_reg, batch_reg.view(-1,1)\n",
    "\n",
    "            # model output\n",
    "            classification_output, hidden_vector = model(batch_feature,batch_user,batch_item) #, regression_output\n",
    "\n",
    "            # FGSM attack\n",
    "            adv_vector = fgsm_attack(model,cls_loss_func,hidden_vector,batch_cls,0.01)\n",
    "            adv_output = model.module.pred_from_hidden(adv_vector) \n",
    "\n",
    "            # calculate loss\n",
    "            adv_loss_func = nn.BCELoss(weight=batch_cls) \n",
    "            adv_loss = beta * adv_loss_func(adv_output,batch_cls) \n",
    "            cls_loss = cls_loss_func(classification_output,batch_cls)\n",
    "            #revenue_loss = alpha * reg_loss_func(regression_output, batch_reg)\n",
    "            loss = cls_loss + adv_loss # + revenue_loss\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if (step+1) % 1000 ==0:  \n",
    "                print(\"CLS loss:%.4f, ADV loss:%.4f, Loss:%.4f\"\\\n",
    "                %(cls_loss.item(),adv_loss.item(),loss.item())) #,revenue_loss.item()\n",
    "                \n",
    "        # evaluate \n",
    "        model.eval()\n",
    "        print(\"Validate at epoch %s\"%(epoch+1))\n",
    "        y_prob, val_loss = model.module.eval_on_batch(valid_loader)\n",
    "        y_pred_tensor = torch.tensor(y_prob).float().to(device)\n",
    "        best_threshold, val_score, roc = torch_threshold(y_pred_tensor,xgb_validy)\n",
    "        overall_f1, auc, precisions, recalls, f1s = metrics(y_pred_tensor,xgb_validy) #, revenues ,revenue_valid\n",
    "        select_best = np.mean(f1s)\n",
    "        print(\"Over-all F1:%.4f, AUC:%.4f, F1-top:%.4f\" % (overall_f1, auc, select_best) )\n",
    "\n",
    "        print(\"Evaluate at epoch %s\"%(epoch+1))\n",
    "        y_prob, val_loss = model.module.eval_on_batch(test_loader)\n",
    "        y_pred_tensor = torch.tensor(y_prob).float().to(device)\n",
    "        overall_f1, auc, precisions, recalls, f1s = metrics(y_pred_tensor,xgb_testy,best_thresh=best_threshold) #, revenuesrevenue_test,\n",
    "        print(\"Over-all F1:%.4f, AUC:%.4f, F1-top:%.4f\" %(overall_f1, auc, np.mean(f1s)) )\n",
    "\n",
    "        # save best model \n",
    "        if select_best > global_best_score:\n",
    "            global_best_score = select_best\n",
    "            torch.save(model,model_path)\n",
    "        \n",
    "         # early stopping \n",
    "        if current_score == None:\n",
    "            current_score = select_best\n",
    "            continue\n",
    "        if select_best < current_score:\n",
    "            current_score = select_best\n",
    "            no_improvement += 1\n",
    "        if no_improvement >= stop_rounds:\n",
    "            print(\"Early stopping...\")\n",
    "            break \n",
    "        if select_best > current_score:\n",
    "            no_improvement = 0\n",
    "            current_score = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1.], dtype=float32)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(xgb_testy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranger optimizer loaded. \n",
      "Gradient Centralization usage = True\n",
      "GC applied to both conv and fc layers\n",
      "CLS loss:0.3367, ADV loss:0.0000, Loss:0.3367\n",
      "CLS loss:0.3392, ADV loss:0.0000, Loss:0.3392\n",
      "Validate at epoch 1\n",
      "CLS loss: 0.4084\n",
      "Over-all F1:0.4161, AUC:0.7257, F1-top:0.1587\n",
      "Evaluate at epoch 1\n",
      "CLS loss: 0.4082\n",
      "Over-all F1:0.4218, AUC:0.7238, F1-top:0.1566\n",
      "CLS loss:0.3812, ADV loss:0.0000, Loss:0.3812\n",
      "CLS loss:0.3858, ADV loss:0.0000, Loss:0.3858\n",
      "Validate at epoch 2\n",
      "CLS loss: 0.4116\n",
      "Over-all F1:0.4158, AUC:0.7255, F1-top:0.1563\n",
      "Evaluate at epoch 2\n",
      "CLS loss: 0.4110\n",
      "Over-all F1:0.4187, AUC:0.7240, F1-top:0.1560\n",
      "CLS loss:0.4270, ADV loss:0.0000, Loss:0.4270\n",
      "CLS loss:0.3225, ADV loss:0.0000, Loss:0.3225\n",
      "Validate at epoch 3\n",
      "CLS loss: 0.4115\n",
      "Over-all F1:0.4264, AUC:0.7290, F1-top:0.1574\n",
      "Evaluate at epoch 3\n",
      "CLS loss: 0.4107\n",
      "Over-all F1:0.4279, AUC:0.7274, F1-top:0.1579\n",
      "CLS loss:0.3607, ADV loss:0.0000, Loss:0.3607\n",
      "CLS loss:0.3217, ADV loss:0.0000, Loss:0.3217\n",
      "Validate at epoch 4\n",
      "CLS loss: 0.4191\n",
      "Over-all F1:0.4113, AUC:0.7177, F1-top:0.1575\n",
      "Evaluate at epoch 4\n",
      "CLS loss: 0.4191\n",
      "Over-all F1:0.4099, AUC:0.7159, F1-top:0.1579\n",
      "CLS loss:0.3284, ADV loss:0.0000, Loss:0.3284\n",
      "CLS loss:0.3563, ADV loss:0.0000, Loss:0.3563\n",
      "Validate at epoch 5\n",
      "CLS loss: 0.4360\n",
      "Over-all F1:0.3808, AUC:0.6865, F1-top:0.1589\n",
      "Evaluate at epoch 5\n",
      "CLS loss: 0.4352\n",
      "Over-all F1:0.3788, AUC:0.6871, F1-top:0.1559\n",
      "CLS loss:0.3395, ADV loss:0.0000, Loss:0.3395\n",
      "CLS loss:0.2866, ADV loss:0.0000, Loss:0.2866\n",
      "Validate at epoch 6\n",
      "CLS loss: 0.4584\n",
      "Over-all F1:0.3807, AUC:0.6734, F1-top:0.1585\n",
      "Evaluate at epoch 6\n",
      "CLS loss: 0.4566\n",
      "Over-all F1:0.3782, AUC:0.6757, F1-top:0.1576\n",
      "CLS loss:0.3475, ADV loss:0.0000, Loss:0.3475\n",
      "CLS loss:0.3411, ADV loss:0.0000, Loss:0.3411\n",
      "Validate at epoch 7\n",
      "CLS loss: 0.4982\n",
      "Over-all F1:0.3625, AUC:0.6553, F1-top:0.1563\n",
      "Evaluate at epoch 7\n",
      "CLS loss: 0.4966\n",
      "Over-all F1:0.3627, AUC:0.6553, F1-top:0.1571\n",
      "CLS loss:0.3383, ADV loss:0.0000, Loss:0.3383\n",
      "CLS loss:0.3324, ADV loss:0.0000, Loss:0.3324\n",
      "Validate at epoch 8\n",
      "CLS loss: 0.5381\n",
      "Over-all F1:0.3547, AUC:0.6421, F1-top:0.1565\n",
      "Evaluate at epoch 8\n",
      "CLS loss: 0.5355\n",
      "Over-all F1:0.3588, AUC:0.6434, F1-top:0.1585\n",
      "CLS loss:0.2252, ADV loss:0.0000, Loss:0.2252\n",
      "CLS loss:0.3591, ADV loss:0.0000, Loss:0.3591\n",
      "Validate at epoch 9\n",
      "CLS loss: 0.5857\n",
      "Over-all F1:0.3586, AUC:0.6314, F1-top:0.1567\n",
      "Evaluate at epoch 9\n",
      "CLS loss: 0.5887\n",
      "Over-all F1:0.3550, AUC:0.6279, F1-top:0.1580\n",
      "CLS loss:0.3280, ADV loss:0.0000, Loss:0.3280\n",
      "CLS loss:0.3284, ADV loss:0.0000, Loss:0.3284\n",
      "Validate at epoch 10\n",
      "CLS loss: 0.6750\n",
      "Over-all F1:0.3291, AUC:0.6038, F1-top:0.1542\n",
      "Evaluate at epoch 10\n",
      "CLS loss: 0.6582\n",
      "Over-all F1:0.3244, AUC:0.6021, F1-top:0.1559\n",
      "CLS loss:0.3028, ADV loss:0.0000, Loss:0.3028\n",
      "CLS loss:0.3071, ADV loss:0.0000, Loss:0.3071\n",
      "Validate at epoch 11\n",
      "CLS loss: 0.7607\n",
      "Over-all F1:0.3447, AUC:0.6031, F1-top:0.1542\n",
      "Evaluate at epoch 11\n",
      "CLS loss: 0.7501\n",
      "Over-all F1:0.3440, AUC:0.6008, F1-top:0.1558\n",
      "CLS loss:0.3167, ADV loss:0.0000, Loss:0.3167\n",
      "CLS loss:0.2549, ADV loss:0.0000, Loss:0.2549\n",
      "Validate at epoch 12\n",
      "CLS loss: 0.7977\n",
      "Over-all F1:0.3282, AUC:0.5992, F1-top:0.1555\n",
      "Evaluate at epoch 12\n",
      "CLS loss: 0.8049\n",
      "Over-all F1:0.3323, AUC:0.5973, F1-top:0.1559\n",
      "CLS loss:0.3036, ADV loss:0.0000, Loss:0.3036\n",
      "CLS loss:0.2905, ADV loss:0.0000, Loss:0.2905\n",
      "Validate at epoch 13\n",
      "CLS loss: 0.9462\n",
      "Over-all F1:0.3201, AUC:0.5718, F1-top:0.1530\n",
      "Evaluate at epoch 13\n",
      "CLS loss: 0.9443\n",
      "Over-all F1:0.3220, AUC:0.5738, F1-top:0.1546\n",
      "CLS loss:0.2985, ADV loss:0.0000, Loss:0.2985\n",
      "CLS loss:0.3104, ADV loss:0.0000, Loss:0.3104\n",
      "Validate at epoch 14\n",
      "CLS loss: 0.9881\n",
      "Over-all F1:0.3156, AUC:0.5775, F1-top:0.1532\n",
      "Evaluate at epoch 14\n",
      "CLS loss: 0.9775\n",
      "Over-all F1:0.3144, AUC:0.5801, F1-top:0.1559\n",
      "CLS loss:0.3267, ADV loss:0.0000, Loss:0.3267\n",
      "CLS loss:0.2922, ADV loss:0.0000, Loss:0.2922\n",
      "Validate at epoch 15\n",
      "CLS loss: 1.1699\n",
      "Over-all F1:0.3224, AUC:0.5728, F1-top:0.1535\n",
      "Evaluate at epoch 15\n",
      "CLS loss: 1.1655\n",
      "Over-all F1:0.3233, AUC:0.5713, F1-top:0.1562\n",
      "CLS loss:0.2830, ADV loss:0.0000, Loss:0.2830\n",
      "CLS loss:0.2545, ADV loss:0.0000, Loss:0.2545\n",
      "Validate at epoch 16\n",
      "CLS loss: 1.1176\n",
      "Over-all F1:0.3186, AUC:0.5761, F1-top:0.1531\n",
      "Evaluate at epoch 16\n",
      "CLS loss: 1.1159\n",
      "Over-all F1:0.3211, AUC:0.5762, F1-top:0.1559\n",
      "CLS loss:0.2315, ADV loss:0.0000, Loss:0.2315\n",
      "CLS loss:0.2780, ADV loss:0.0000, Loss:0.2780\n",
      "Validate at epoch 17\n",
      "CLS loss: 1.2814\n",
      "Over-all F1:0.3203, AUC:0.5769, F1-top:0.1534\n",
      "Evaluate at epoch 17\n",
      "CLS loss: 1.2950\n",
      "Over-all F1:0.3235, AUC:0.5792, F1-top:0.1569\n",
      "CLS loss:0.2776, ADV loss:0.0000, Loss:0.2776\n",
      "CLS loss:0.2471, ADV loss:0.0000, Loss:0.2471\n",
      "Validate at epoch 18\n",
      "CLS loss: 1.4075\n",
      "Over-all F1:0.3180, AUC:0.5720, F1-top:0.1525\n",
      "Evaluate at epoch 18\n",
      "CLS loss: 1.4089\n",
      "Over-all F1:0.3222, AUC:0.5724, F1-top:0.1558\n",
      "CLS loss:0.2518, ADV loss:0.0000, Loss:0.2518\n",
      "CLS loss:0.3008, ADV loss:0.0000, Loss:0.3008\n",
      "Validate at epoch 19\n",
      "CLS loss: 1.4246\n",
      "Over-all F1:0.3093, AUC:0.5542, F1-top:0.1515\n",
      "Evaluate at epoch 19\n",
      "CLS loss: 1.4007\n",
      "Over-all F1:0.3096, AUC:0.5562, F1-top:0.1539\n",
      "CLS loss:0.2384, ADV loss:0.0000, Loss:0.2384\n",
      "CLS loss:0.3105, ADV loss:0.0000, Loss:0.3105\n",
      "Validate at epoch 20\n",
      "CLS loss: 1.5183\n",
      "Over-all F1:0.3080, AUC:0.5518, F1-top:0.1515\n",
      "Evaluate at epoch 20\n",
      "CLS loss: 1.5188\n",
      "Over-all F1:0.3132, AUC:0.5531, F1-top:0.1527\n"
     ]
    }
   ],
   "source": [
    "train(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4. Evaluation <a id='part4'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(save_model):\n",
    "    print()\n",
    "    print(\"--------Evaluating DATE model---------\")\n",
    "    # create best model\n",
    "    best_model = torch.load(model_path)\n",
    "    best_model.eval()\n",
    "\n",
    "    # get threshold\n",
    "    y_prob, val_loss = best_model.module.eval_on_batch(valid_loader)\n",
    "    best_threshold, val_score, roc = torch_threshold(y_prob,xgb_validy)\n",
    "\n",
    "    # predict test \n",
    "    y_prob, val_loss = best_model.module.eval_on_batch(test_loader)\n",
    "    overall_f1, auc, precisions, recalls, f1s = metrics(y_prob,xgb_testy,best_threshold) #, revenues,revenue_test\n",
    "    best_score = f1s[0]\n",
    "    #os.system(\"rm %s\"%model_path)\n",
    "    if save_model:\n",
    "        scroed_name = \"./saved_models/%s_%.4f.pkl\" % (model_name,overall_f1)\n",
    "        torch.save(best_model,scroed_name)\n",
    "    \n",
    "    return overall_f1, auc, precisions, recalls, f1s #, revenues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------Evaluating DATE model---------\n",
      "CLS loss: 0.4360\n",
      "CLS loss: 0.4352\n"
     ]
    }
   ],
   "source": [
    "overall_f1, auc, precisions, recalls, f1s = evaluate(model_path) #, revenues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving result... ./results/full.csv\n",
      "\n",
      "Metrics:\n",
      "f1:0.3788 auc:0.6871\n",
      "Pr@1:0.4230 Pr@2:0.4054 Pr@5:0.4043 Pr@10:0.3955\n",
      "Re@1:0.0257 Re@2:0.0493 Re@5:0.1228 Re@10:0.2403\n"
     ]
    }
   ],
   "source": [
    "# save result\n",
    "output_file =  \"./results/full.csv\"\n",
    "print(\"Saving result...\",output_file)\n",
    "with open(output_file, 'a') as ff:\n",
    "    # print(args,file=ff)\n",
    "    print()\n",
    "    print(\"\"\"Metrics:\\nf1:%.4f auc:%.4f\\nPr@1:%.4f Pr@2:%.4f Pr@5:%.4f Pr@10:%.4f\\nRe@1:%.4f Re@2:%.4f Re@5:%.4f Re@10:%.4f\"\"\" \\\n",
    "          % (overall_f1, auc,\\\n",
    "             precisions[0],precisions[1],precisions[2],precisions[3],\\\n",
    "             recalls[0],recalls[1],recalls[2],recalls[3] #,\\evenues[0],revenues[1],revenues[2],revenues[3]\n",
    "            ),\n",
    "        ) \n",
    "    output_metric = [16,overall_f1,auc] + precisions + recalls # + revenues\n",
    "    output_metric = list(map(str,output_metric))\n",
    "    print(\" \".join(output_metric),file=ff)\n",
    "        \n",
    "    # print(\"Model:%s epoch:%d dim:%d lr:%f l2:%f beta:%f heads:%d fusion:%s activation:%s\"\n",
    "    #       % (model_name, epochs, dim, lr, weight_decay, beta, head_num,fusion,act),file=ff) \n",
    "    # print(\"\"\"Metrics:\\nf1:%.4f auc:%.4f\\nPr@1:%.4f Pr@2:%.4f Pr@5:%.4f Pr@10:%.4f\\nRe@1:%.4f Re@2:%.4f Re@5:%.4f Re@10:%.4f\\nRev@1:%.4f Rev@2:%.4f Rev@5:%.4f Rev@10:%.4f\"\"\"  \\\n",
    "    #       % (overall_f1, auc,\\\n",
    "    #          precisions[0],precisions[1],precisions[2],precisions[3],\\\n",
    "    #          recalls[0],recalls[1],recalls[2],recalls[3],\\\n",
    "    #          revenues[0],revenues[1],revenues[2],revenues[3]\n",
    "    #          ),\n",
    "    #          file=ff)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
